<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="hcYDp6Fp83zSq_ebw4L0fyy9A85xh7nJVXRc0YIrplg">
  <meta name="msvalidate.01" content="6607AE0DF3E7D531126AB256C9D013F2">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Comic+Sans+MS:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.36/fancybox/fancybox.css" integrity="sha256-zM8WXtG4eUn7dKKNMTuoWZub++VnSfaOpA/8PJfvTBo=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"0b1t.tech","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="神经网络的概念 想象一下，我们想教计算机识别图片中的猫，但是我们不能为它编写一套硬性的规则（“如果它有尖耳朵、胡须和毛茸茸的尾巴，那它就是一只猫”），因为现实世界中的猫品种各异，而且猫娘也符合上面的规则。 神经网络采用了一种不同的方法：从样本中学习。就像我们通过看到真实的各种猫之后才知道这种动物叫 “猫” 一样，神经网络也会处理成千上万张标记好的图片（“这是猫”，“这不是猫”），并通过这种方法逐渐">
<meta property="og:type" content="article">
<meta property="og:title" content="从零开始的神经网络学习 (一)：浅析神经网络">
<meta property="og:url" content="https://0b1t.tech/Machine-learning/36604.html">
<meta property="og:site_name" content="0wnerD1ed&#39;s blog">
<meta property="og:description" content="神经网络的概念 想象一下，我们想教计算机识别图片中的猫，但是我们不能为它编写一套硬性的规则（“如果它有尖耳朵、胡须和毛茸茸的尾巴，那它就是一只猫”），因为现实世界中的猫品种各异，而且猫娘也符合上面的规则。 神经网络采用了一种不同的方法：从样本中学习。就像我们通过看到真实的各种猫之后才知道这种动物叫 “猫” 一样，神经网络也会处理成千上万张标记好的图片（“这是猫”，“这不是猫”），并通过这种方法逐渐">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/1-NN.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/3-weights.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/4-weightedSum.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/5-weightsAndBias.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/6-matrices1.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/7-matrices2.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/8-cost.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/9-gradientDescent.png">
<meta property="og:image" content="https://0b1t.tech/Machine-learning/36604/10-backpropagation.png">
<meta property="article:published_time" content="2025-06-24T06:21:00.000Z">
<meta property="article:modified_time" content="2025-06-25T09:03:36.220Z">
<meta property="article:author" content="0wnerD1ed">
<meta property="article:tag" content="NN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://0b1t.tech/Machine-learning/36604/1-NN.png">


<link rel="canonical" href="https://0b1t.tech/Machine-learning/36604.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://0b1t.tech/Machine-learning/36604.html","path":"Machine-learning/36604.html","title":"从零开始的神经网络学习 (一)：浅析神经网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>从零开始的神经网络学习 (一)：浅析神经网络 | 0wnerD1ed's blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.36/fancybox/fancybox.umd.js" integrity="sha256-hiUEBwFEpLF6DlB8sGXlKo4kPZ46Ui4qGpd0vrVkOm4=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>




  <script src="/js/third-party/fancybox.js" defer></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>


<link rel="dns-prefetch" href="blog-waline-mauve.vercel.app">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">0wnerD1ed's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">神经网络的概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.</span> <span class="nav-text">前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E5%92%8C%E6%BF%80%E6%B4%BB%E5%80%BC"><span class="nav-number">2.1.</span> <span class="nav-text">神经元和激活值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%9D%83%E9%87%8D"><span class="nav-number">2.2.</span> <span class="nav-text">连接和权重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%BF%80%E6%B4%BB%E5%80%BC"><span class="nav-number">2.3.</span> <span class="nav-text">计算激活值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97"><span class="nav-number">2.4.</span> <span class="nav-text">矩阵运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%81%8F%E7%BD%AE%E5%92%8C%E5%B9%BF%E6%92%AD-a-deeper-look-at-bias-and-broadcasting"><span class="nav-number">2.4.1.</span> <span class="nav-text">深入理解偏置和广播
(A Deeper Look at Bias and Broadcasting)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">3.2.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">3.3.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E4%B8%AA%E6%9D%83%E9%87%8D%E7%9A%84%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E8%B4%A3%E4%BB%BB%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.3.1.</span> <span class="nav-text">单个权重的的链式法则责任推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E8%B4%A3%E4%BB%BB%E4%BC%A0%E9%80%92"><span class="nav-number">3.3.2.</span> <span class="nav-text">多层神经网络的链式法则责任传递</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="nav-number">3.4.</span> <span class="nav-text">总结训练循环</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-python-%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.</span> <span class="nav-text">一个简单的 Python 实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="0wnerD1ed"
      src="https://avatars.githubusercontent.com/u/59904423?s=400&u=e2d918cc9959b82599bb4dbfcebb8b912880b9dc&v=4">
  <p class="site-author-name" itemprop="name">0wnerD1ed</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/0wnerDied" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;0wnerDied" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:atndko@outlook.sg" title="E-Mail → mailto:atndko@outlook.sg" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://t.me/OwnerDiedddd" title="Telegram → https:&#x2F;&#x2F;t.me&#x2F;OwnerDiedddd" rel="noopener me" target="_blank"><i class="fab fa-telegram fa-fw"></i></a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://0b1t.tech/Machine-learning/36604.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/59904423?s=400&u=e2d918cc9959b82599bb4dbfcebb8b912880b9dc&v=4">
      <meta itemprop="name" content="0wnerD1ed">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="0wnerD1ed's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="从零开始的神经网络学习 (一)：浅析神经网络 | 0wnerD1ed's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从零开始的神经网络学习 (一)：浅析神经网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-24 14:21:00" itemprop="dateCreated datePublished" datetime="2025-06-24T14:21:00+08:00">2025-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-25 17:03:36" itemprop="dateModified" datetime="2025-06-25T17:03:36+08:00">2025-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-learning/" itemprop="url" rel="index"><span itemprop="name">Machine learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/Machine-learning/36604.html#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/Machine-learning/36604.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>42 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="神经网络的概念">神经网络的概念</h2>
<p>想象一下，我们想教计算机识别图片中的猫，但是我们不能为它编写一套硬性的规则（“如果它有尖耳朵、胡须和毛茸茸的尾巴，那它就是一只猫”），因为现实世界中的猫品种各异，<del>而且猫娘也符合上面的规则</del>。</p>
<p>神经网络采用了一种不同的方法：<strong>从样本中学习</strong>。就像我们通过看到真实的各种猫之后才知道这种动物叫 “猫” 一样，神经网络也会处理成千上万张标记好的图片（“这是猫”，“这不是猫”），并通过这种方法逐渐 “学会” 识别猫的视觉模式。</p>
<p>从技术上讲，神经网络是一种受人脑结构启发的计算模型。它由许多简单的处理单元 —— <strong>神经元
(Neuron)</strong> 组成，这些神经元组织在不同的<strong>层
(Layer)</strong> 中。</p>
<p>一个经典的神经网络有三层：</p>
<ul>
<li><strong>输入层 (Input Layer)</strong>:
负责接收最原始的数据。例如，对于一张图片，每个神经元可能对应图片中的一个像素值。</li>
<li><strong>隐藏层 (Hidden Layers)</strong>:
位于输入层和输出层之间。这些是网络进行大部分 “思考” 的地方。一个神经网络可以没有隐藏层，也可以有很多个。层数越多，网络通常越 “深”。</li>
<li><strong>输出层 (Output Layer)</strong>:
产生最终的结果。例如，在猫识别任务中，输出层可能只有一个神经元，其输出值在 0 到 1 之间，表示图片是猫的概率。</li>
</ul>
<img src="/Machine-learning/36604/1-NN.png" class="" title="NN">
<h2 id="前向传播">前向传播</h2>
<p>数据在网络中流动并最终得到一个预测结果的过程就被称为<strong>前向传播
(Forward Propagation)</strong>。</p>
<h3 id="神经元和激活值">神经元和激活值</h3>
<p>简单来说，神经元就是持有一个数的简单单元，这个数称为<strong>激活值
(Activation)</strong>。输入层的神经元激活值就是我们输入的数据本身。对于其他层的神经元，它的激活值需要通过计算得出。</p>
<h3 id="连接和权重">连接和权重</h3>
<p>不同层的神经元之间通过连接进行通信。每个连接都有一个<strong>权重
(Weight)</strong>，这个数代表了连接的强度和重要性。</p>
<ul>
<li>一个较大的正权重意味着前一个神经元的激活会对后一个神经元产生强烈的 “兴奋” 作用。</li>
<li>一个较大的负权重则相反，表示会产生强烈的 “抑制” 作用。</li>
<li>接近于零的权重意味着前一个神经元对后一个神经元几乎没有影响。</li>
</ul>
<p><strong>训练神经网络的本质，就是调整这些权重的值。</strong></p>
<img src="/Machine-learning/36604/3-weights.png" class="" title="weights">
<h3 id="计算激活值">计算激活值</h3>
<p>一个神经元激活值的计算分两步：</p>
<ul>
<li><p><strong>计算加权和 (Weighted Sum)</strong>:
神经元接收来自前一层所有神经元的输入。它将每个输入的激活值乘以它们之间连接的权重，然后将所有结果相加。</p>
<img src="/Machine-learning/36604/4-weightedSum.png" class="" title="wSum">
<p>在加权和以外，我们还会加上一个额外的数字，叫做<strong>偏置
(Bias)</strong>。偏置的作用是提供一个可调的 “基础激活水平”。你可以把它看作是，在没有任何输入的情况下，一个神经元有多容易被激活。比如，一个高的偏置使得神经元更容易被激活。</p>
<p>所以，一个神经元的 “预激活值”（我们称之为
<code>z</code>）的完整公式是：</p>
<p><span class="math display">$$ z = \sum_{i=1}^{n}(w_i a_i) + bias
$$</span></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">weights = [<span class="number">0.2</span>, <span class="number">0.8</span>, -<span class="number">0.5</span>]</span><br><span class="line">bias = <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">z = inputs[<span class="number">0</span>] * weights[<span class="number">0</span>] + inputs[<span class="number">1</span>] * weights[<span class="number">1</span>] + inputs[<span class="number">2</span>] * weights[<span class="number">2</span>] + bias</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="comment"># z = 2.3</span></span><br></pre></td></tr></tbody></table></figure>
<img src="/Machine-learning/36604/5-weightsAndBias.png" class="" title="wBias"></li>
<li><p><strong>应用激活函数</strong>:
如果我们只用加权和，那么无论网络有多少层，它本质上都只是在做一个简单的线性变换。这限制了它学习复杂模式的能力。</p>
<p>为了引入<strong>非线性
(non-linearity)</strong>，我们需要一个<strong>激活函数</strong>。它接收上一步计算出的
<code>z</code> 值，并输出最终的激活值 <code>a</code>。</p>
<p>常见的激活函数有很多，这里我们介绍 <strong>ReLU (Rectified Linear
Unit，修正线性单元)</strong>。它的规则非常简单：</p>
<ul>
<li>如果输入 <code>z</code> 大于 0，输出就是 <code>z</code> 本身。</li>
<li>如果输入 <code>z</code> 小于或等于 0，输出就是 0。</li>
</ul>
<p>其数学表达式为：</p>
<p><span class="math display"><em>f</em>(<em>z</em>) = max (0, <em>z</em>)</span></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">activation = relu(<span class="number">2.3</span>)</span><br><span class="line">activation_neg = relu(-<span class="number">1.2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"ReLU(2.3) = <span class="subst">{activation}</span>, ReLU(-1.2) = <span class="subst">{activation_neg}</span>"</span>)</span><br><span class="line"><span class="comment"># ReLU(2.3) = 2.3, ReLU(-1.2) = 0.0</span></span><br></pre></td></tr></tbody></table></figure>
<p>ReLU
之所以受欢迎，是因为它在计算上很高效，并且在实践中能帮助网络更有效地学习。</p></li>
</ul>
<h3 id="矩阵运算">矩阵运算</h3>
<p>当神经网络变得很大时，逐个计算每个神经元的加权和会非常慢。幸运的是，我们可以使用线性代数中的矩阵运算来一次性完成一整层神经元的计算。</p>
<blockquote>
<p>注：下文所有涉及的 <code>@</code> 运算符为 Python 3.5+
的矩阵乘法（matmul），推荐用于神经网络等线性代数场景。<code>np.dot</code>
在一维时是内积，二维时是矩阵乘法，但 <code>@</code>
始终表示矩阵乘法，更直观。</p>
</blockquote>
<p>我们可以把：</p>
<ul>
<li>一层的输入激活值看作一个向量 <code>A</code>。</li>
<li>连接到下一层的所有权重组织成一个矩阵 <code>W</code>。</li>
<li>所有偏置组成一个向量 <code>B</code>。</li>
</ul>
<p>那么，下一层所有神经元的预激活值 <code>Z</code>
就可以通过一个简单的公式计算出来：</p>
<p><code>Z = A @ W + B</code></p>
<p>其中 <code>@</code> 代表矩阵乘法。</p>
<p>假设我们有一个包含 2 个样本的批次（batch），每个样本有 3 个特征（输入），我们要将其传入一个有 4 个神经元的隐藏层。</p>
<ul>
<li><code>A</code> (激活值矩阵) 的形状是 <code>(2, 3)</code></li>
<li><code>W</code> (权重矩阵) 的形状是 <code>(3, 4)</code></li>
<li><code>B</code> (偏置向量) 的形状是 <code>(1, 4)</code> (它会被广播到
<code>(2, 4)</code>)</li>
<li><code>Z</code> (输出预激活矩阵) 的形状将是 <code>(2, 4)</code></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">activations = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">weights = np.random.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"权重: "</span>, weights)</span><br><span class="line">biases = np.random.rand(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"偏置: "</span>, biases)</span><br><span class="line"></span><br><span class="line">Z = activations @ weights + biases</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Z 的形状:"</span>, Z.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Z 的值:\n"</span>, Z)</span><br><span class="line"><span class="comment"># 权重:  [[0.08746301 0.34148947 0.2321176  0.49574324]</span></span><br><span class="line"><span class="comment">#  [0.69313075 0.77251665 0.48220435 0.38541945]</span></span><br><span class="line"><span class="comment">#  [0.48551131 0.84943177 0.05167356 0.03890675]]</span></span><br><span class="line"><span class="comment"># 偏置:  [[0.05122642 0.2860902  0.75485931 0.75830747]]</span></span><br><span class="line"><span class="comment"># Z 的形状: (2, 4)</span></span><br><span class="line"><span class="comment"># Z 的值:</span></span><br><span class="line"><span class="comment">#  [[ 2.98148486  4.72090828  2.1064063   2.14160985]</span></span><br><span class="line"><span class="comment">#  [ 6.77980008 10.61122193  4.40439285  4.90181816]]</span></span><br></pre></td></tr></tbody></table></figure>
<p>这正是现代深度学习框架 (如 TensorFlow 和
PyTorch) 在底层所做的事情，它能极大地利用 GPU 的并行计算能力。</p>
<h4 id="深入理解偏置和广播-a-deeper-look-at-bias-and-broadcasting">深入理解偏置和广播
(A Deeper Look at Bias and Broadcasting)</h4>
<p>在上面 <a href="#矩阵运算">2.4</a> 节中，你可能会问：“为什么偏置
<code>B</code> 的形状是 <code>(1, 4)</code>，却能和形状为
<code>(2, 4)</code> 的矩阵 <code>A · W</code> 相加呢？”</p>
<p>这要归功于 NumPy 的一个强大特性：<strong>广播
(Broadcasting)</strong>。</p>
<p>我们的隐藏层有 4 个神经元，所以我们有 4 个对应的偏置值（存储在
<code>B</code> 中）。当我们一次性处理一个包含 2 个样本的批次
(batch) 时，这 4 个偏置值需要被分别应用到<strong>每一个</strong>样本的计算结果上。当
NumPy 看到你要将一个 <code>(2, 4)</code> 矩阵和一个 <code>(1, 4)</code>
向量相加时，它会自动将这个 <code>(1, 4)</code>
的行向量 “拉伸” 或 “复制” 成一个 <code>(2, 4)</code>
的矩阵，使其形状匹配，然后再执行元素间的加法。NumPy
并不会真的创建这个大矩阵的副本，而是通过一种更巧妙的内部机制来完成计算，节省了大量内存和计算时间。</p>
<p>简单来说，广播机制让我们能用简洁的代码，将同一组偏置高效地应用到一批数据中的所有样本上。</p>
<p>我们可以通过一个具体的代码示例来观察广播是如何工作的:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A_dot_W = np.array([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>], [<span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>]])</span><br><span class="line"></span><br><span class="line">B = np.array([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"A · W 的形状:"</span>, A_dot_W.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"B 的形状:"</span>, B.shape)</span><br><span class="line"></span><br><span class="line">Z = A_dot_W + B</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n--- 执行 A_dot_W + B ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"在广播后，B 表现得像下面这个 (2, 4) 的矩阵:"</span>)</span><br><span class="line"></span><br><span class="line">broadcasted_B_for_demo = np.tile(B, (<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(broadcasted_B_for_demo)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n最终结果 Z 的形状:"</span>, Z.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"最终结果 Z 的值:\n"</span>, Z)</span><br><span class="line"><span class="comment"># A · W 的形状: (2, 4)</span></span><br><span class="line"><span class="comment"># B 的形状: (1, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 执行 A_dot_W + B ---</span></span><br><span class="line"><span class="comment"># 在广播后，B 表现得像下面这个 (2, 4) 的矩阵:</span></span><br><span class="line"><span class="comment"># [[0.1 0.2 0.3 0.4]</span></span><br><span class="line"><span class="comment">#  [0.1 0.2 0.3 0.4]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终结果 Z 的形状: (2, 4)</span></span><br><span class="line"><span class="comment"># 最终结果 Z 的值:</span></span><br><span class="line"><span class="comment">#  [[10.1 20.2 30.3 40.4]</span></span><br><span class="line"><span class="comment">#  [50.1 60.2 70.3 80.4]]</span></span><br></pre></td></tr></tbody></table></figure>
<p>我们可以清晰地看到 <code>B</code> 是如何被 “拉伸” 以匹配
<code>A_dot_W</code> 的形状，从而完成加法运算的。</p>
<img src="/Machine-learning/36604/6-matrices1.png" class="" title="matrices1">
<img src="/Machine-learning/36604/7-matrices2.png" class="" title="matrices2">
<h2 id="训练过程">训练过程</h2>
<p>我们已经知道数据是如何在网络中流动的 (前向传播)，但网络是如何 “学习”
—— 也就是如何找到正确的权重和偏置的呢？这个过程叫做<strong>训练
(Training)</strong>。</p>
<p>训练就像一个反馈循环：</p>
<ul>
<li><strong>预测</strong>：让网络根据当前的权重和偏置进行一次前向传播，得到一个预测结果。</li>
<li><strong>评估</strong>：将预测结果与真实的答案进行比较，计算出 “误差” 有多大。</li>
<li><strong>学习</strong>：根据误差，反向调整网络中的所有权重和偏置，目标是让下一次的误差变得更小。</li>
</ul>
<h3 id="损失函数">损失函数</h3>
<p>为了评估网络的表现，我们需要一个<strong>损失函数 (Loss
Function)</strong>。这会量化预测值和真实值之间的差距。</p>
<p>一个常用的损失函数是<strong>均方误差 (Mean Squared Error,
MSE)</strong>：</p>
<ul>
<li>计算每个输出神经元的预测值与真实值之差。</li>
<li>将这个差值平方（这样可以确保结果是正数，并且对较大的误差给予更大的 “惩罚”）。</li>
<li>将所有输出神经元的平方差加起来，并取平均值。</li>
</ul>
<p>即 <code>loss = mean((pred - true)²)</code></p>
<p>其数学公式为 (<span class="math inline"><em>k</em></span>
是输出神经元的数量):</p>
<p><span class="math display">$$ C = \frac{1}{m} \sum_{i=1}^{m}
\sum_{j=1}^{k} (y_{pred, ij} - y_{true, ij})^2 $$</span></p>
<p>用代码实现： </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y_true = np.array([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">y_pred = np.array([[<span class="number">0.1</span>, <span class="number">0.9</span>], [<span class="number">0.8</span>, <span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = mse_loss(y_true, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"loss = "</span>, loss)</span><br><span class="line"><span class="comment"># loss = 0.024999999999999994</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>我们的目标就是通过调整权重和偏置，让这个损失值<strong>尽可能小</strong>。</p>
<blockquote>
<p>实际上，对于分类任务，更常用的是 softmax
激活配合交叉熵损失（cross-entropy loss）。这里以 MSE
为例只是因为演示方便，后续章节会详细介绍分类场景的常用方法。</p>
</blockquote>
<img src="/Machine-learning/36604/8-cost.png" class="" title="cost">
<h3 id="梯度下降">梯度下降</h3>
<p>想象一下，你正站在一座大山 (损失函数)
上，蒙着眼睛，你的目标是走到山谷的最低点 (最小损失)。你会怎么做？
你可能会伸出脚，感受一下哪个方向是下坡最陡峭的，然后朝那个方向迈一小步。重复这个过程，你最终会到达谷底。</p>
<p>这就是<strong>梯度下降 (Gradient
Descent)</strong> 算法的核心思想。</p>
<ul>
<li><strong>梯度
(Gradient)</strong> 在数学上指向了函数值增长最快的方向 (也就是上坡最陡的方向)。</li>
<li>因此，我们只需要沿着<strong>负梯度</strong> (梯度的反方向) 前进，就能最快地降低损失值。
<img src="/Machine-learning/36604/9-gradientDescent.png" class="" title="gradientDescent"></li>
</ul>
<p>我们还需要一个<strong>学习率 (Learning
Rate)</strong>，它决定了我们沿着下坡方向 “迈出的步子有多大”。</p>
<ul>
<li><strong>学习率太小</strong>：下山速度会非常慢。</li>
<li><strong>学习率太大</strong>：我们可能会在谷底来回 “跨过”，永远无法到达最低点。</li>
</ul>
<p>选择一个合适的学习率是训练神经网络中的一个重要技巧。</p>
<p>参数更新的规则可以表示为： <span class="math display">$$ w_{new} =
w_{old} - \eta \frac{\partial C}{\partial w} $$</span> 其中
<code>η</code> 是学习率，<code>∂C/∂w</code> 是损失对权重 <code>w</code>
的梯度。</p>
<h3 id="反向传播">反向传播</h3>
<p>梯度下降告诉了我们要下山，但没有告诉我们网络中成千上万个权重和偏置具体要如何改变才能实现下山。</p>
<p><strong>反向传播
(Backpropagation)</strong> 就是解决这个问题的算法。它的核心思想是 “分配责任”。</p>
<p>在一次预测后，我们得到了总的误差。反向传播会从输出层开始，反向地通过网络，计算出每个权重和偏置对这个总误差 “贡献” 了多少责任。</p>
<h4 id="单个权重的的链式法则责任推导">单个权重的的链式法则责任推导</h4>
<p>这个过程依赖于微积分中的<strong>链式法则</strong>。我们想知道改变一个权重
<code>w</code> 会如何影响最终的损失 <code>C</code> (即梯度
<code>∂C/∂w</code>)。这个梯度可以被分解为几个部分的乘积：</p>
<p><span class="math display">$$ \frac{\partial C}{\partial w} =
\frac{\partial C}{\partial a} \times \frac{\partial a}{\partial z}
\times \frac{\partial z}{\partial w} $$</span></p>
<p>下面我们将拆解这个公式，把它想象成一个 “责任追踪” 的过程。我们的目标
<code>∂C/∂w</code> 是要算出权重 <code>w</code> 应该为最终的误差
<code>C</code> 负多少责任。</p>
<p>链式法则告诉我们，这个总责任可以分解为一连串局部责任的乘积：</p>
<ol type="1">
<li><strong><code>∂z/∂w</code>：权重 <code>w</code> 对神经元输入
<code>z</code> 的责任。</strong>
<ul>
<li>回顾公式 <code>z = w * a_prev + ...</code>，<code>w</code>
的影响大小，完全取决于它所乘的那个输入值 <code>a_prev</code>。如果
<code>a_prev</code> 很大，<code>w</code> 的一点小变化就会被放大；如果
<code>a_prev</code> 是 0，<code>w</code> 再怎么变也影响不了
<code>z</code>。<strong>所以，这部分的责任就是
<code>a_prev</code>。</strong></li>
</ul></li>
<li><strong><code>∂a/∂z</code>：神经元输入 <code>z</code> 对其输出
<code>a</code> 的责任。</strong>
<ul>
<li>对于 ReLU 函数，如果 <code>z</code> 本来就大于 0，那 <code>z</code>
的变化会直接通过，责任是 1。如果 <code>z</code> 小于等于 0，那
<code>z</code> 再怎么变，输出 <code>a</code>
都是 0，没有变化，那么责任是 0。<strong>所以，这部分的责任是激活函数的导数，它决定了梯度能否继续向后流动。</strong></li>
</ul></li>
<li><strong><code>∂C/∂a</code>：神经元输出 <code>a</code> 对最终总误差
<code>C</code> 的责任。</strong>
<ul>
<li>这是最关键的一步，也是 “反向传播” 中 “传播” 的体现。一个神经元的输出
<code>a</code>
会影响到下一层所有与它相连的神经元，进而通过整个网络，最终影响到总误差
<code>C</code>。我们无法直接计算这个责任。<strong>所以，这个责任值必须从下一层 “传播” 回来。</strong>
它代表了所有来自 “前方”（更靠近输出层）的误差信号汇集到 <code>a</code>
这一点上的总和。</li>
</ul></li>
</ol>
<p>综上，为了计算一个权重的梯度（<code>∂C/∂w</code>），我们把这三份 “责任” 相乘：(来自前一层的输入
<code>a_prev</code>) × (激活函数的导数) ×
(从后一层传回来的误差)。这就是反向传播的核心计算。</p>
<h4 id="多层神经网络的链式法则责任传递">多层神经网络的链式法则责任传递</h4>
<p>上述推导是针对单个权重的梯度，便于理解本质。而在实际代码实现中，一般用矩阵运算一次性计算整层的所有权重和偏置的梯度，这样程序的运算效率更高。代码中的
<code>dC_da</code>、<code>da_dz</code>、<code>dC_dz</code>、<code>dC_dw</code>、<code>dC_db</code>、<code>dC_da_prev</code>
等变量，都是批量（矩阵 / 向量）形式的 “责任” 传递，和理论推导一一对应，只是用矩阵方式高效实现。</p>
<p>在多层神经网络中，反向传播的 “责任” 会一层层传递。我们以第 <span class="math inline"><em>l</em></span> 层为例，假设：</p>
<ul>
<li><span class="math inline"><em>a</em><sup>[<em>l</em>]</sup></span>：第 <span class="math inline"><em>l</em></span> 层的输出（激活值）</li>
<li><span class="math inline"><em>z</em><sup>[<em>l</em>]</sup></span>：第 <span class="math inline"><em>l</em></span> 层的加权和</li>
<li><span class="math inline"><em> W</em><sup>[<em>l</em>]</sup></span>、<span class="math inline"><em>b</em><sup>[<em>l</em>]</sup></span>：第 <span class="math inline"><em>l</em></span> 层的权重和偏置</li>
</ul>
<p>前向传播： <span class="math display">$$
\begin{align}
z^{[l]} &amp;= W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} &amp;= f(z^{[l]})
\end{align}
$$</span></p>
<p>反向传播：</p>
<p>第一个 “误差信号” <span class="math inline">$\frac{\partial
C}{\partial
a^{[L]}}$</span>（损失对输出层激活的梯度）是直接由损失函数和真实标签计算得到。例如对于均方误差（MSE）损失
<span class="math inline">$C = \frac{1}{2}(a^{[L]} - y)^2$</span>，有
<span class="math inline">$\frac{\partial C}{\partial a^{[L]}} = a^{[L]}
- y$</span>。</p>
<p>“真实标签” 指的是训练数据中每个样本的正确答案，也叫 “目标值” 或 “ground
truth”。比如：</p>
<ul>
<li>图像分类任务中，真实标签就是图片实际对应的类别（如 “猫” 或 “狗”）。</li>
<li>回归任务中，真实标签就是希望模型预测出来的那个数值。</li>
</ul>
<p>在反向传播时，假设我们已经得到了 <span class="math inline">$\frac{\partial C}{\partial
a^{[l]}}$</span>，则：</p>
<ol type="1">
<li><p>计算 <span class="math inline">$\frac{\partial C}{\partial
z^{[l]}}$</span>。其中，符号 <span class="math inline">⊙</span>
表示逐元素乘法： <span class="math display">$$
\delta^{[l]} = \frac{\partial C}{\partial z^{[l]}} = \frac{\partial
C}{\partial a^{[l]}} \odot f'(z^{[l]})
$$</span></p></li>
<li><p> 计算本层参数的梯度，这里的 <span class="math inline"><em>m</em></span> 是 batch size： <span class="math display">$$
\begin{align}
\frac{\partial C}{\partial W^{[l]}} &amp;= \delta^{[l]} (a^{[l-1]})^T \\
\frac{\partial C}{\partial b^{[l]}} &amp;= \sum_{i=1}^m \delta^{[l]}_i
\end{align}
$$</span></p></li>
<li><p> 计算传递给前一层的 “责任”： <span class="math display">$$
\frac{\partial C}{\partial a^{[l-1]}} = (W^{[l]})^T \delta^{[l]}
$$</span></p></li>
</ol>
<p>这样，误差信号 <span class="math inline"><em>δ</em></span>
就一层层地传递回去。</p>
<img src="/Machine-learning/36604/10-backpropagation.png" class="" title="backpropagation">
<p><strong>变量对应关系</strong>：</p>
<table>
<colgroup>
<col style="width: 39%">
<col style="width: 19%">
<col style="width: 41%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">公式符号</th>
<th style="text-align: left;">代码变量名</th>
<th style="text-align: left;">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial a^{[l]}}$</span></td>
<td style="text-align: left;"><code>dC_da</code></td>
<td style="text-align: left;">损失对本层激活的梯度 (来自 <span class="math inline"><em>l</em> + 1</span> 层)</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"><em>f</em><sup>′</sup>(<em>z</em><sup>[<em>l</em>]</sup>)</span></td>
<td style="text-align: left;"><code>da_dz</code></td>
<td style="text-align: left;">激活函数对加权和的导数</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"><em> δ</em><sup>[<em>l</em>]</sup></span></td>
<td style="text-align: left;"><code>dC_dz</code></td>
<td style="text-align: left;">损失对本层加权和的梯度</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial W^{[l]}}$</span></td>
<td style="text-align: left;"><code>dC_dw</code></td>
<td style="text-align: left;">损失对本层权重的梯度</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial b^{[l]}}$</span></td>
<td style="text-align: left;"><code>dC_db</code></td>
<td style="text-align: left;">损失对本层偏置的梯度</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial a^{[l-1]}}$</span></td>
<td style="text-align: left;"><code>dC_da_prev</code></td>
<td style="text-align: left;">损失对前一层激活的梯度 (传递回 <span class="math inline"><em>l</em> − 1</span> 层)</td>
</tr>
</tbody>
</table>
<p>下面的代码段详细解释了每个变量的含义和它与理论公式的对应关系。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    执行反向传播，计算并应用梯度。</span></span><br><span class="line"><span class="string">    dC_da: 损失函数对本神经元输出 a 的梯度 (从下一层传来)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 1. 计算 da/dz (激活函数对z的梯度)</span></span><br><span class="line">    <span class="comment"># 对应链式法则中的 ∂a/∂z，形状与 z 相同</span></span><br><span class="line">    da_dz = <span class="variable language_">self</span>.relu_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 计算 dC/dz (损失对z的梯度) = dC/da * da/dz</span></span><br><span class="line">    <span class="comment"># 对应链式法则中的 ∂C/∂a × ∂a/∂z，形状与 z 相同</span></span><br><span class="line">    dC_dz = dC_da * da_dz</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 计算 dC/dw (损失对权重的梯度) = dC/dz * dz/dw</span></span><br><span class="line">    <span class="comment"># dz/dw = a_prev，dC/dw = a_prev.T @ dC/dz</span></span><br><span class="line">    <span class="comment"># 这里 a_prev 是输入，dC/dz 是“误差信号”，矩阵乘法一次性算出所有权重的梯度</span></span><br><span class="line">    dC_dw = <span class="variable language_">self</span>.last_input.T @ dC_dz</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 计算 dC/db (损失对偏置的梯度) = dC/dz * dz/db</span></span><br><span class="line">    <span class="comment"># dz/db = 1，dC/db = sum(dC/dz)</span></span><br><span class="line">    dC_db = np.<span class="built_in">sum</span>(dC_dz, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 计算 dC/da_prev (损失对前一层激活值的梯度)，用于传给前一层</span></span><br><span class="line">    <span class="comment"># dC/da_prev = dC/dz * dz/da_prev，dz/da_prev = weights</span></span><br><span class="line">    dC_da_prev = dC_dz @ <span class="variable language_">self</span>.weights.T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 根据梯度更新权重和偏置</span></span><br><span class="line">    <span class="variable language_">self</span>.weights -= learning_rate * dC_dw</span><br><span class="line">    <span class="variable language_">self</span>.bias -= learning_rate * dC_db</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回 dC/da_prev，传递给前一层继续反向传播</span></span><br><span class="line">    <span class="keyword">return</span> dC_da_prev</span><br></pre></td></tr></tbody></table></figure>
<h3 id="总结训练循环">总结训练循环</h3>
<p>于是，我们可以完整地描述训练过程了，这个过程会重复很多次 (称为
<strong>Epochs</strong>):</p>
<ol type="1">
<li><strong>前向传播</strong>：将一批训练数据输入网络，计算出预测值。</li>
<li><strong>计算损失</strong>：使用损失函数，比较预测值和真实值，得到损失。</li>
<li><strong>反向传播</strong>：从损失出发，反向计算出网络中每个权重和偏置的梯度。</li>
<li><strong>更新参数</strong>：使用梯度下降，根据梯度和学习率，对所有权重和偏置进行一次微小的更新。</li>
</ol>
<p>经过成千上万次的迭代，网络的权重和偏置会逐渐收敛到一组能够很好地完成任务的值。网络就 “学会” 了辨认猫猫。</p>
<h2 id="一个简单的-python-实现">一个简单的 Python 实现</h2>
<p>下面是一个用 Python 和 NumPy 从零开始实现的简单神经网络。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单个神经元。管理自己的权重、偏置，并执行前向和后向计算。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs</span>):</span><br><span class="line">        <span class="comment"># 随机初始化权重，乘以一个小数以防止初始值过大</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span></span><br><span class="line">        <span class="comment"># 初始化偏置为0</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 存储计算过程中的中间值，用于反向传播</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_z = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""ReLU 激活函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(<span class="number">0</span>, z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu_derivative</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""ReLU 激活函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.where(z &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行前向传播：z = a @ w + b, a_out = relu(z)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = activations</span><br><span class="line">        <span class="comment"># 计算加权和 z</span></span><br><span class="line">        z = activations @ <span class="variable language_">self</span>.weights + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="variable language_">self</span>.last_z = z</span><br><span class="line">        <span class="comment"># 应用激活函数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行反向传播，计算并应用梯度。</span></span><br><span class="line"><span class="string">        dC_da: 损失函数对本神经元输出 a 的梯度 (从下一层传来)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 1. 计算 da/dz (激活函数对z的梯度)</span></span><br><span class="line">        <span class="comment"># 对应链式法则中的 ∂a/∂z，形状与 z 相同</span></span><br><span class="line">        da_dz = <span class="variable language_">self</span>.relu_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算 dC/dz (损失对z的梯度) = dC/da * da/dz</span></span><br><span class="line">        <span class="comment"># 对应链式法则中的 ∂C/∂a × ∂a/∂z，形状与 z 相同</span></span><br><span class="line">        dC_dz = dC_da * da_dz</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 计算 dC/dw (损失对权重的梯度) = dC/dz * dz/dw</span></span><br><span class="line">        <span class="comment"># dz/dw = a_prev，dC/dw = a_prev.T @ dC/dz</span></span><br><span class="line">        <span class="comment"># 这里 a_prev 是输入，dC/dz 是“误差信号”，矩阵乘法一次性算出所有权重的梯度</span></span><br><span class="line">        dC_dw = <span class="variable language_">self</span>.last_input.T @ dC_dz</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 计算 dC/db (损失对偏置的梯度) = dC/dz * dz/db</span></span><br><span class="line">        <span class="comment"># dz/db = 1，dC/db = sum(dC/dz)</span></span><br><span class="line">        dC_db = np.<span class="built_in">sum</span>(dC_dz, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 计算 dC/da_prev (损失对前一层激活值的梯度)，用于传给前一层</span></span><br><span class="line">        <span class="comment"># dC/da_prev = dC/dz * dz/da_prev，dz/da_prev = weights</span></span><br><span class="line">        dC_da_prev = dC_dz @ <span class="variable language_">self</span>.weights.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 根据梯度更新权重和偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.weights -= learning_rate * dC_dw</span><br><span class="line">        <span class="variable language_">self</span>.bias -= learning_rate * dC_db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回 dC/da_prev，传递给前一层继续反向传播</span></span><br><span class="line">        <span class="keyword">return</span> dC_da_prev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一层神经元。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_neurons, num_inputs_per_neuron</span>):</span><br><span class="line">        <span class="variable language_">self</span>.neurons = [Neuron(num_inputs_per_neuron) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_neurons)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""对层中所有神经元执行前向传播"""</span></span><br><span class="line">        <span class="comment"># hstack 用于水平堆叠输出，形成一个 (batch_size, num_neurons) 的矩阵</span></span><br><span class="line">        <span class="keyword">return</span> np.hstack([neuron.forward(activations) <span class="keyword">for</span> neuron <span class="keyword">in</span> <span class="variable language_">self</span>.neurons])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">        <span class="string">"""对层中所有神经元执行反向传播"""</span></span><br><span class="line">        <span class="comment"># dC_da 的形状是 (batch_size, num_neurons)</span></span><br><span class="line">        <span class="comment"># 我们需要为每个神经元传入对应的梯度 dC_da[:, [i]]</span></span><br><span class="line">        <span class="comment"># 然后将所有神经元返回的 dC/da_prev 相加，得到传给前一层的总梯度</span></span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(</span><br><span class="line">            [</span><br><span class="line">                neuron.backward(dC_da[:, [i]], learning_rate)</span><br><span class="line">                <span class="keyword">for</span> i, neuron <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.neurons)</span><br><span class="line">            ],</span><br><span class="line">            axis=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    完整的神经网络模型。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_sizes</span>):</span><br><span class="line">        <span class="comment"># layer_sizes 是一个列表，例如 [784, 128, 10]</span></span><br><span class="line">        <span class="comment"># 表示输入层784个节点，隐藏层128个，输出层10个</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(Layer(layer_sizes[i + <span class="number">1</span>], layer_sizes[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""对所有层执行前向传播"""</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            activations = layer.forward(activations)</span><br><span class="line">        <span class="keyword">return</span> activations</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="string">"""均方误差损失函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">derivative_mse_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="string">"""均方误差损失函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * (y_pred - y_true) / y_true.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs, learning_rate, batch_size=<span class="number">32</span></span>):</span><br><span class="line">        <span class="string">"""训练神经网络"""</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 使用小批量梯度下降</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(X), batch_size):</span><br><span class="line">                X_batch = X[i : i + batch_size]</span><br><span class="line">                y_batch = y[i : i + batch_size]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 1. 前向传播</span></span><br><span class="line">                outputs = <span class="variable language_">self</span>.forward(X_batch)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 2. 计算损失</span></span><br><span class="line">                loss = <span class="variable language_">self</span>.mse_loss(y_batch, outputs)</span><br><span class="line">                total_loss += loss * <span class="built_in">len</span>(X_batch)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 3. 计算输出层的梯度</span></span><br><span class="line">                output_gradient = <span class="variable language_">self</span>.derivative_mse_loss(y_batch, outputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 4. 反向传播</span></span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.layers):</span><br><span class="line">                    output_gradient = layer.backward(output_gradient, learning_rate)</span><br><span class="line"></span><br><span class="line">            avg_loss = total_loss / <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{epochs}</span>, Loss: <span class="subst">{avg_loss:<span class="number">.6</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">"""用训练好的网络进行预测"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.forward(X)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>Ian Goodfellow, Yoshua Bengio, Aaron Courville. <a target="_blank" rel="noopener" href="https://www.deeplearningbook.org/">Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/DorsaRoh/Machine-Learning">Machine
learning - DorsaRoh</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>0wnerD1ed
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://0b1t.tech/Machine-learning/36604.html" title="从零开始的神经网络学习 (一)：浅析神经网络">https://0b1t.tech/Machine-learning/36604.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NN/" rel="tag"><i class="fa fa-tag"></i> NN</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/kernel/50584.html" rel="prev" title="浅析 eBPF">
                  <i class="fa fa-angle-left"></i> 浅析 eBPF
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Machine-learning/38196.html" rel="next" title="从零开始的神经网络学习 (二)：实用技巧与进阶">
                  从零开始的神经网络学习 (二)：实用技巧与进阶 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">0wnerD1ed</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/0wnerDied" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"blog-waline-mauve.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"locale":{"placeholder":"善语结善缘，恶语伤人心"},"pageSize":30,"comment_count":true,"dark":"html[data-darkmode]","meta":[],"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/Machine-learning/36604.html"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script>
document.addEventListener('DOMContentLoaded', function() {
  // 等待Waline元素加载完成
  const checkWaline = setInterval(function() {
    const walineEl = document.querySelector('.wl-panel');
    if (walineEl) {
      clearInterval(checkWaline);
      
      // 监听暗黑模式变化，强制更新评论区样式
      if (typeof window.darkmode !== 'undefined') {
        window.darkmode.addEventListener('change', function(event) {
          setTimeout(function() {
            const isDark = document.body.classList.contains('darkmode--activated');
            
            if (isDark) {
              // 直接修改DOM元素样式
              document.querySelectorAll('.wl-panel, .wl-card').forEach(el => {
                el.style.setProperty('background-color', '#222', 'important');
                el.style.setProperty('border-color', '#444', 'important');
              });
              
              document.querySelectorAll('.wl-editor, .wl-input, .wl-textarea').forEach(el => {
                el.style.setProperty('background-color', '#333', 'important');
                el.style.setProperty('color', '#ddd', 'important');
                el.style.setProperty('border-color', '#555', 'important');
              });
              
              document.querySelectorAll('.wl-content, .wl-meta, .wl-meta span, .wl-meta a').forEach(el => {
                el.style.setProperty('color', '#ccc', 'important');
              });
            }
          }, 100);
        });
      }
    }
  }, 100);
});
</script>

</body>
</html>
