<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>uCore ch3 实验报告</title>
    <url>/uCore/28113.html</url>
    <content><![CDATA[<h2 id="本章任务">本章任务</h2>
<h3 id="获取任务信息">获取任务信息</h3>
<p>ch3
中，我们的系统已经能够支持多个任务分时轮流运行，我们希望引入一个新的系统调用
<code>sys_trace</code> （syscall ID:
410）用来追踪当前任务系统调用的历史信息、并进行任务内存的读写。定义如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">sys_trace</span><span class="params">(<span class="type">int</span> trace_request, <span class="type">unsigned</span> <span class="type">long</span> id, <span class="type">uint8_t</span> data)</span>;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li><p>syscall ID: 410</p></li>
<li><p> 调用规范：</p>
<ul>
<li>这个系统调用有三种功能，根据 trace_request
的值不同，执行不同的操作：</li>
<li>如果 trace_request 为 0，则 id 应被视作 uint8_t * ，表示读取当前任务
id 地址处一个字节的无符号整数值。此时应忽略 data 参数。返回值为 id
地址处的值。</li>
<li>如果 trace_request 为 1，则 id 应被视作 uint8_t * ，表示写入 data
（作为 uint8_t，即只考虑最低位的一个字节）到该用户程序 id
地址处。返回值应为 0。</li>
<li>如果 trace_request 为 2，表示查询当前任务调用编号为 id
的系统调用的次数，返回值为这个调用次数。本次调用也计入统计 。</li>
<li>否则，忽略其他参数，返回值为 -1。</li>
</ul></li>
<li><p>说明：</p>
<ul>
<li>uint8_t
是 C 语言中的标准类型，表示一个无符号的 8 位整数。在代码中你可能需要使用
uint8 替代。</li>
<li>你可能会注意到，这个调用的读写并不安全，使用不当可能导致崩溃。这是因为在下一章节实现地址空间之前，系统中缺乏隔离机制。所以我们
不要求你实现安全检查机制，只需通过测试用例即可。</li>
<li>你还可能注意到，这个系统调用读写本任务内存的功能并不是很有用。这是因为作业的灵感来源
syscall 主要依靠 trace
功能追踪其他任务的信息，但在本章节我们还没有进程、线程等概念，所以简化了操作，只要求追踪自身的信息。</li>
</ul></li>
</ul>
<h3 id="完成任务">完成任务</h3>
<p>下面是我对代码的修改，导出提交：</p>
<figure class="highlight patch"><table><tbody><tr><td class="code"><pre><span class="line">From b1836f667109436153481ae6f338a6a50a19e232 Mon Sep 17 00:00:00 2001</span><br><span class="line">From: 0wnerDied &lt;z1281552865@gmail.com&gt;</span><br><span class="line">Date: Wed, 23 Jul 2025 22:57:41 +0800</span><br><span class="line">Subject: [PATCH] chapter3 practice</span><br><span class="line"></span><br><span class="line">Signed-off-by: 0wnerDied &lt;z1281552865@gmail.com&gt;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"> os/loader.c      |  1 +</span><br><span class="line"> os/proc.c        |  1 +</span><br><span class="line"> os/proc.h        |  1 +</span><br><span class="line"> os/syscall.c     | 22 ++++++++++++++++++++++</span><br><span class="line"> os/syscall_ids.h |  1 +</span><br><span class="line"> 5 files changed, 26 insertions(+)</span><br><span class="line"></span><br><span class="line"><span class="comment">diff --git a/os/loader.c b/os/loader.c</span></span><br><span class="line"><span class="comment">index be9f10a..72b49ed 100644</span></span><br><span class="line"><span class="comment">--- a/os/loader.c</span></span><br><span class="line"><span class="comment">+++ b/os/loader.c</span></span><br><span class="line"><span class="meta">@@ -51,6 +51,7 @@</span> int run_all_app()</span><br><span class="line"> 		/*</span><br><span class="line"> 		* LAB1: you may need to initialize your new fields of proc here</span><br><span class="line"> 		*/</span><br><span class="line"><span class="addition">+		memset(p-&gt;syscall_cnt, 0, sizeof(p-&gt;syscall_cnt));</span></span><br><span class="line"> 	}</span><br><span class="line"> 	return 0;</span><br><span class="line"> }</span><br><span class="line">\ No newline at end of file</span><br><span class="line"><span class="comment">diff --git a/os/proc.c b/os/proc.c</span></span><br><span class="line"><span class="comment">index 84240a8..c98b49e 100644</span></span><br><span class="line"><span class="comment">--- a/os/proc.c</span></span><br><span class="line"><span class="comment">+++ b/os/proc.c</span></span><br><span class="line"><span class="meta">@@ -34,6 +34,7 @@</span> void proc_init(void)</span><br><span class="line"> 		/*</span><br><span class="line"> 		* LAB1: you may need to initialize your new fields of proc here</span><br><span class="line"> 		*/</span><br><span class="line"><span class="addition">+		memset(p-&gt;syscall_cnt, 0, sizeof(p-&gt;syscall_cnt));</span></span><br><span class="line"> 	}</span><br><span class="line"> 	idle.kstack = (uint64)boot_stack_top;</span><br><span class="line"> 	idle.pid = 0;</span><br><span class="line"><span class="comment">diff --git a/os/proc.h b/os/proc.h</span></span><br><span class="line"><span class="comment">index f27369a..74cfe1c 100644</span></span><br><span class="line"><span class="comment">--- a/os/proc.h</span></span><br><span class="line"><span class="comment">+++ b/os/proc.h</span></span><br><span class="line"><span class="meta">@@ -38,6 +38,7 @@</span> struct proc {</span><br><span class="line"> 	/*</span><br><span class="line"> 	* LAB1: you may need to add some new fields here</span><br><span class="line"> 	*/</span><br><span class="line"><span class="addition">+	int syscall_cnt[512];</span></span><br><span class="line"> };</span><br><span class="line"> </span><br><span class="line"> struct proc *curr_proc();</span><br><span class="line"><span class="comment">diff --git a/os/syscall.c b/os/syscall.c</span></span><br><span class="line"><span class="comment">index 3225031..50b0035 100644</span></span><br><span class="line"><span class="comment">--- a/os/syscall.c</span></span><br><span class="line"><span class="comment">+++ b/os/syscall.c</span></span><br><span class="line"><span class="meta">@@ -39,6 +39,22 @@</span> uint64 sys_gettimeofday(TimeVal *val, int _tz)</span><br><span class="line"> /*</span><br><span class="line"> * LAB1: you may need to define sys_trace here</span><br><span class="line"> */</span><br><span class="line"><span class="addition">+int sys_trace(int trace_request, unsigned long id, uint8 data)</span></span><br><span class="line"><span class="addition">+{</span></span><br><span class="line"><span class="addition">+	struct proc *p = curr_proc();</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+	switch (trace_request) {</span></span><br><span class="line"><span class="addition">+	case 0:</span></span><br><span class="line"><span class="addition">+		return *(uint8 *)id;</span></span><br><span class="line"><span class="addition">+	case 1:</span></span><br><span class="line"><span class="addition">+		*(uint8 *)id = data;</span></span><br><span class="line"><span class="addition">+		return 0;</span></span><br><span class="line"><span class="addition">+	case 2:</span></span><br><span class="line"><span class="addition">+		return (id &lt; 512) ? p-&gt;syscall_cnt[id] : -1;</span></span><br><span class="line"><span class="addition">+	default:</span></span><br><span class="line"><span class="addition">+		return -1;</span></span><br><span class="line"><span class="addition">+	}</span></span><br><span class="line"><span class="addition">+}</span></span><br><span class="line"> </span><br><span class="line"> extern char trap_page[];</span><br><span class="line"> </span><br><span class="line"><span class="meta">@@ -53,6 +69,9 @@</span> void syscall()</span><br><span class="line"> 	/*</span><br><span class="line"> 	* LAB1: you may need to update syscall counter here</span><br><span class="line"> 	*/</span><br><span class="line"><span class="addition">+	struct proc *p = curr_proc();</span></span><br><span class="line"><span class="addition">+	(id &lt; 512) ? p-&gt;syscall_cnt[id]++ : 0;</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"> 	switch (id) {</span><br><span class="line"> 	case SYS_write:</span><br><span class="line"> 		ret = sys_write(args[0], (char *)args[1], args[2]);</span><br><span class="line"><span class="meta">@@ -69,6 +88,9 @@</span> void syscall()</span><br><span class="line"> 	/*</span><br><span class="line"> 	* LAB1: you may need to add SYS_trace case here</span><br><span class="line"> 	*/</span><br><span class="line"><span class="addition">+	case SYS_trace:</span></span><br><span class="line"><span class="addition">+		ret = sys_trace(args[0], args[1], args[2]);</span></span><br><span class="line"><span class="addition">+		break;</span></span><br><span class="line"> 	default:</span><br><span class="line"> 		ret = -1;</span><br><span class="line"> 		errorf("unknown syscall %d", id);</span><br><span class="line"><span class="comment">diff --git a/os/syscall_ids.h b/os/syscall_ids.h</span></span><br><span class="line"><span class="comment">index e6fac51..06d8c00 100644</span></span><br><span class="line"><span class="comment">--- a/os/syscall_ids.h</span></span><br><span class="line"><span class="comment">+++ b/os/syscall_ids.h</span></span><br><span class="line"><span class="meta">@@ -280,6 +280,7 @@</span></span><br><span class="line"> /*</span><br><span class="line"> * LAB1: you may need to define SYS_trace here</span><br><span class="line"> */</span><br><span class="line"><span class="addition">+#define SYS_trace 410</span></span><br><span class="line"> #define SYS_pidfd_send_signal 424</span><br><span class="line"> #define SYS_io_uring_setup 425</span><br><span class="line"> #define SYS_io_uring_enter 426</span><br><span class="line"><span class="deletion">-- </span></span><br><span class="line">2.25.1</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>然后 <code>make test</code> 检查即可</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">[rustsbi] RustSBI version 0.3.0-alpha.2, adapting to RISC-V SBI v1.0.0</span><br><span class="line">.______       __    __      _______.___________.  _______..______   __</span><br><span class="line">|   _  \     |  |  |  |    /       |           | /       ||   _  \ |  |</span><br><span class="line">|  |_)  |    |  |  |  |   |   (----`---|  |----`|   (----`|  |_)  ||  |</span><br><span class="line">|      /     |  |  |  |    \   \       |  |      \   \    |   _  &lt; |  |</span><br><span class="line">|  |\  \----.|  `--'  |.----)   |      |  |  .----)   |   |  |_)  ||  |</span><br><span class="line">| _| `._____| \______/ |_______/       |__|  |_______/    |______/ |__|</span><br><span class="line">[rustsbi] Implementation     : RustSBI-QEMU Version 0.2.0-alpha.2</span><br><span class="line">[rustsbi] Platform Name      : riscv-virtio,qemu</span><br><span class="line">[rustsbi] Platform SMP       : 1</span><br><span class="line">[rustsbi] Platform Memory    : 0x80000000..0x88000000</span><br><span class="line">[rustsbi] Boot HART          : 0</span><br><span class="line">[rustsbi] Device Tree Region : 0x87000000..0x87000ef2</span><br><span class="line">[rustsbi] Firmware Address   : 0x80000000</span><br><span class="line">[rustsbi] Supervisor Address : 0x80200000</span><br><span class="line">[rustsbi] pmp01: 0x00000000..0x80000000 (-wr)</span><br><span class="line">[rustsbi] pmp02: 0x80000000..0x80200000 (---)</span><br><span class="line">[rustsbi] pmp03: 0x80200000..0x88000000 (xwr)</span><br><span class="line">[rustsbi] pmp04: 0x88000000..0x00000000 (-wr)</span><br><span class="line">Hello world from user mode program!</span><br><span class="line">Test hello_world OK!</span><br><span class="line">3^10000=5079</span><br><span class="line">3^20000=8202</span><br><span class="line">3^30000=8824</span><br><span class="line">3^40000=5750</span><br><span class="line">3^50000=3824</span><br><span class="line">3^60000=8516</span><br><span class="line">3^70000=2510</span><br><span class="line">3^80000=9379</span><br><span class="line">3^90000=2621</span><br><span class="line">3^100000=2749</span><br><span class="line">Test power OK!</span><br><span class="line">get_time OK! 11</span><br><span class="line">current time_msec = 11</span><br><span class="line">AAAAAAAAAA [1/5]</span><br><span class="line">CCCCCCCCCC [1/5]</span><br><span class="line">BBBBBBBBBB [1/5]</span><br><span class="line">AAAAAAAAAA [2/5]</span><br><span class="line">CCCCCCCCCC [2/5]</span><br><span class="line">BBBBBBBBBB [2/5]</span><br><span class="line">AAAAAAAAAA [3/5]</span><br><span class="line">CCCCCCCCCC [3/5]</span><br><span class="line">BBBBBBBBBB [3/5]</span><br><span class="line">AAAAAAAAAA [4/5]</span><br><span class="line">CCCCCCCCCC [4/5]</span><br><span class="line">BBBBBBBBBB [4/5]</span><br><span class="line">AAAAAAAAAA [5/5]</span><br><span class="line">CCCCCCCCCC [5/5]</span><br><span class="line">BBBBBBBBBB [5/5]</span><br><span class="line">Test write A OK!</span><br><span class="line">Test write C OK!</span><br><span class="line">Test write B OK!</span><br><span class="line">time_msec = 112 after sleeping 100 ticks, delta = 101ms!</span><br><span class="line">Test sleep1 passed!</span><br><span class="line">string from task trace test</span><br><span class="line">Test trace OK!</span><br><span class="line">Test sleep OK!</span><br><span class="line">[PANIC 5] os/loader.c:14: all apps over</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到 <code>Test trace OK!</code>，修改成功通过了测试。</p>
<h2 id="问答作业">问答作业</h2>
<ol type="1">
<li><p>正确进入 U 态后，程序的特征还应有：使用 S 态特权指令，访问 S
态寄存器后会报错。请同学们可以自行测试这些内容（参考
前三个测例，描述程序出错行为，同时注意注明你使用的 sbi
及其版本。</p></li>
<li><p>请结合用例理解 trampoline.S 中两个函数 <code>userret</code> 和
<code>uservec</code> 的作用，并回答如下几个问题:</p>
<ol type="1">
<li><p>L79: 刚进入 <code>userret</code>
时，<code>a0</code>、<code>a1</code> 分别代表了什么值。</p></li>
<li><p>L87-L88: <code>sfence</code>
指令有何作用？为什么要执行该指令，当前章节中，删掉该指令会导致错误吗？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">csrw satp, a1</span><br><span class="line">sfence.vma zero, zero</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>L96-L125: 为何注释中说要除去 a0？哪一个地址代表 a0？现在 a0
的值存在何处？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># restore all but a0 from TRAPFRAME</span><br><span class="line">ld ra, 40(a0)</span><br><span class="line">ld sp, 48(a0)</span><br><span class="line">ld t5, 272(a0)</span><br><span class="line">ld t6, 280(a0)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p><code>userret</code>：中发生状态切换在哪一条指令？为何执行之后会进入用户态？</p></li>
<li><p>L29： 执行之后，<code>a0</code> 和 <code>sscratch</code>
中各是什么值，为什么？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">csrrw a0, sscratch, a0</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>L32-L61: 从 trapframe
第几项开始保存？为什么？是否从该项开始保存了所有的值，如果不是，为什么？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sd ra, 40(a0)</span><br><span class="line">sd sp, 48(a0)</span><br><span class="line">...</span><br><span class="line">sd t5, 272(a0)</span><br><span class="line">sd t6, 280(a0)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>进入 S 态是哪一条指令发生的？</p></li>
<li><p>L75-L76: <code>ld t0, 16(a0)</code>
执行之后，<code>t0</code> 中的值是什么，解释该值的由来？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ld t0, 16(a0)</span><br><span class="line">jr t0</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ol></li>
</ol>
<h2 id="问答作业解答">问答作业解答</h2>
<ol type="1">
<li><p>正确进入 U 态后，程序的特征还应有：使用 S 态特权指令，访问 S
态寄存器后会报错。请同学们可以自行测试这些内容（参考前三个测例
，描述程序出错行为，同时注意注明你使用的 sbi 及其版本。</p>
<p>执行 <code>make test CHAPTER=2_bad</code> 后如下：</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">[rustsbi] RustSBI version 0.3.0-alpha.2, adapting to RISC-V SBI v1.0.0</span><br><span class="line">.______       __    __      _______.___________.  _______..______   __</span><br><span class="line">|   _  \     |  |  |  |    /       |           | /       ||   _  \ |  |</span><br><span class="line">|  |_)  |    |  |  |  |   |   (----`---|  |----`|   (----`|  |_)  ||  |</span><br><span class="line">|      /     |  |  |  |    \   \       |  |      \   \    |   _  &lt; |  |</span><br><span class="line">|  |\  \----.|  `--'  |.----)   |      |  |  .----)   |   |  |_)  ||  |</span><br><span class="line">| _| `._____| \______/ |_______/       |__|  |_______/    |______/ |__|</span><br><span class="line">[rustsbi] Implementation     : RustSBI-QEMU Version 0.2.0-alpha.2</span><br><span class="line">[rustsbi] Platform Name      : riscv-virtio,qemu</span><br><span class="line">[rustsbi] Platform SMP       : 1</span><br><span class="line">[rustsbi] Platform Memory    : 0x80000000..0x88000000</span><br><span class="line">[rustsbi] Boot HART          : 0</span><br><span class="line">[rustsbi] Device Tree Region : 0x87000000..0x87000ef2</span><br><span class="line">[rustsbi] Firmware Address   : 0x80000000</span><br><span class="line">[rustsbi] Supervisor Address : 0x80200000</span><br><span class="line">[rustsbi] pmp01: 0x00000000..0x80000000 (-wr)</span><br><span class="line">[rustsbi] pmp02: 0x80000000..0x80200000 (---)</span><br><span class="line">[rustsbi] pmp03: 0x80200000..0x88000000 (xwr)</span><br><span class="line">[rustsbi] pmp04: 0x88000000..0x00000000 (-wr)</span><br><span class="line">hello wrold!</span><br><span class="line">[ERROR 0]unknown trap: 0x0000000000000007, stval = 0x0000000000000000 sepc = 0x0000000080400004</span><br><span class="line">[ERROR 0]IllegalInstruction in application, epc = 0x0000000080400004, core dumped.</span><br><span class="line">[ERROR 0]IllegalInstruction in application, epc = 0x0000000080400004, core dumped.</span><br><span class="line">ALL DONE</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>可以看到项目使用 RustSBI，版本为 0.3.0-alpha.2。</p>
<p>别忘记 <code>trap</code> 类异常：</p>
<p></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Exception</span> {</span></span><br><span class="line">  InstructionMisaligned = <span class="number">0</span>,</span><br><span class="line">  InstructionAccessFault = <span class="number">1</span>,</span><br><span class="line">  IllegalInstruction = <span class="number">2</span>,</span><br><span class="line">  Breakpoint = <span class="number">3</span>,</span><br><span class="line">  LoadMisaligned = <span class="number">4</span>,</span><br><span class="line">  LoadAccessFault = <span class="number">5</span>,</span><br><span class="line">  StoreMisaligned = <span class="number">6</span>,</span><br><span class="line">  StoreAccessFault = <span class="number">7</span>,</span><br><span class="line">  UserEnvCall = <span class="number">8</span>,</span><br><span class="line">  SupervisorEnvCall = <span class="number">9</span>,</span><br><span class="line">  MachineEnvCall = <span class="number">11</span>,</span><br><span class="line">  InstructionPageFault = <span class="number">12</span>,</span><br><span class="line">  LoadPageFault = <span class="number">13</span>,</span><br><span class="line">  StorePageFault = <span class="number">15</span>,</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>三个测例如下：</p>
<p></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// __ch2_bad_address.c</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">  <span class="type">int</span> *p = (<span class="type">int</span> *)<span class="number">0</span>;</span><br><span class="line">  *p = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>这是一个空指针解引用的测例，U 态程序尝试访问 S 态，显然运行会触发
<code>Store/AMO access fault</code> 异常，所以我们可以看到
<code>unknown trap: 0x0000000000000007</code>。</p>
<p></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// __ch2_bad_instruction.c</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">"sret"</span>)</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>这是一个使用 S 态特权指令的测例，因为 U 态不能执行 S
态特权指令，所以会触发 <code>Illegal instruction</code> 异常。</p>
<p></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// __ch2_bad_register.c</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">  uint64 x;</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">"csrr %0, sstatus"</span> : <span class="string">"=r"</span>(x))</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>这是一个访问 S 态寄存器的测例，同样地，U 态不能访问 S 态 CSR
寄存器，所以会触发 <code>Illegal instruction</code> 异常。</p>
<p>通过上面的测试可以看到，成功输出了
<code>hello wrold!</code>，即用户态程序可以正常运行，且 RustSBI
正常工作，但存在对 U 态的特权级别限制，所以这几个测例会报错。</p></li>
<li><p>请结合用例理解 trampoline.S 中两个函数 <code>userret</code> 和
<code>uservec</code> 的作用，并回答如下几个问题:</p>
<ol type="1">
<li><p>L79: 刚进入 <code>userret</code>
时，<code>a0</code>、<code>a1</code> 分别代表了什么值。</p>
<p>从注释中可以看到 </p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">userret:</span><br><span class="line">    # userret(TRAPFRAME, pagetable)</span><br><span class="line">    # switch from kernel to user.</span><br><span class="line">    # usertrapret() calls here.</span><br><span class="line">    # a0: TRAPFRAME, in user page table.</span><br><span class="line">    # a1: user page table, for satp.</span><br></pre></td></tr></tbody></table></figure> 所以 <code>a0</code> 指向 TRAPFRAME
的地址；<code>a1</code> 是用户页表的物理地址，将被设置为寄存器
<code>satp</code> 的值。<p></p></li>
<li><p>L87-L88: <code>sfence</code>
指令有何作用？为什么要执行该指令，当前章节中，删掉该指令会导致错误吗？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">csrw satp, a1</span><br><span class="line">sfence.vma zero, zero</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>刷新 TLB。当修改 <code>satp</code> 寄存器切换页表时，TLB
中可能还缓存着旧页表的地址转换，必须刷新 TLB
确保后续的地址转换使用新页表。当前章节中删去不会出错，这里还不涉及用户页表切换。</p></li>
<li><p>L96-L125: 为何注释中说要除去 <code>a0</code>？哪一个地址代表
<code>a0</code>？现在 <code>a0</code> 的值存在何处？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># restore all but a0 from TRAPFRAME</span><br><span class="line">ld ra, 40(a0)</span><br><span class="line">ld sp, 48(a0)</span><br><span class="line">...</span><br><span class="line">ld t5, 272(a0)</span><br><span class="line">ld t6, 280(a0)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>当前 <code>a0</code> 存储的是 TRAPFRAME
的地址，需要用它访问保存的寄存器值，如果现在恢复
<code>a0</code>，就无法继续访问 TRAPFRAME 了。<code>a0</code>
的原始值保存在 <code>112(a0)</code> 位置，也就是 TRAPFRAME 中的
<code>a0</code> 字段。在恢复过程中会先被加载到 <code>sscratch</code>
寄存器，最后通过 L128：<code>csrrw a0, sscratch, a0</code> 指令完成
<code>a0</code> 的恢复。</p></li>
<li><p><code>userret</code>：中发生状态切换在哪一条指令？为何执行之后会进入用户态？</p>
<p><code>sret</code>。<code>sret</code> 指令会根据 <code>sstatus</code>
寄存器的 <code>SPP</code> 位决定返回的特权级，在
<code>usertrapret()</code> 中，<code>x &amp;= ~SSTATUS_SPP;</code>
已经设置了 <code>sstatus.SPP = 0</code>，表示返回用户态。</p></li>
<li><p>L29： 执行之后，<code>a0</code> 和 <code>sscratch</code>
中各是什么值，为什么？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">csrrw a0, sscratch, a0</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><code>csrrw</code> 是原子交换指令，同时读取和写入 <code>CSR</code>
寄存器，所以 <code>a0</code> 是 TRAPFRAME 的地址，<code>sscratch</code>
中是用户程序的原始 <code>a0</code> 值。</p></li>
<li><p>L32-L61: 从 trapframe
第几项开始保存？为什么？是否从该项开始保存了所有的值，如果不是，为什么？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sd ra, 40(a0)</span><br><span class="line">sd sp, 48(a0)</span><br><span class="line">...</span><br><span class="line">sd t5, 272(a0)</span><br><span class="line">sd t6, 280(a0)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>从 <code>ra</code> 开始保存，<code>ra</code> 的偏移量为 40，<span class="math inline">40 ÷ 8 = 5</span>，所以从 <code>trapframe</code>
第六项开始保存。并没有保存所有的值，<code>a0</code> 没有保存，因为此时
<code>a0</code> 指向 <code>trapframe</code>，其原始值通过
<code>csrrw a0, sscratch, a0</code> 保存在 <code>sscratch</code>
中，后续在 <code>trapframe</code> 偏移量 112 处，也就是
<code>trapframe</code> 中的 <code>a0</code> 字段保存。</p></li>
<li><p>进入 S 态是哪一条指令发生的？</p>
<p>用户程序中的 <code>ecall</code>。</p></li>
<li><p>L75-L76: <code>ld t0, 16(a0)</code>
执行之后，<code>t0</code> 中的值是什么，解释该值的由来？</p>
<p></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ld t0, 16(a0)</span><br><span class="line">jr t0</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// os/trap.c</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">usertrapret</span><span class="params">(<span class="keyword">struct</span> trapframe *trapframe, uint64 kstack)</span></span><br><span class="line">{</span><br><span class="line">  trapframe-&gt;kernel_satp = r_satp(); <span class="comment">// kernel page table</span></span><br><span class="line">  trapframe-&gt;kernel_sp = kstack + PGSIZE; <span class="comment">// process's kernel stack</span></span><br><span class="line">  trapframe-&gt;kernel_trap = (uint64)usertrap;</span><br><span class="line">  trapframe-&gt;kernel_hartid = r_tp(); <span class="comment">// hartid for cpuid()</span></span><br><span class="line"></span><br><span class="line">  w_sepc(trapframe-&gt;epc);</span><br><span class="line">  <span class="comment">// set up the registers that trampoline.S's sret will use</span></span><br><span class="line">  <span class="comment">// to get to user space.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// set S Previous Privilege mode to User.</span></span><br><span class="line">  uint64 x = r_sstatus();</span><br><span class="line">  x &amp;= ~SSTATUS_SPP; <span class="comment">// clear SPP to 0 for user mode</span></span><br><span class="line">  x |= SSTATUS_SPIE; <span class="comment">// enable interrupts in user mode</span></span><br><span class="line">  w_sstatus(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// tell trampoline.S the user page table to switch to.</span></span><br><span class="line">  <span class="comment">// uint64 satp = MAKE_SATP(p-&gt;pagetable);</span></span><br><span class="line">  userret((uint64)trapframe);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>可以看到
<code>trapframe-&gt;kernel_trap = (uint64)usertrap;</code>，所以
<code>t0</code> 中的值是 <code>usertrap</code>
函数的地址。当系统调用完成准备返回用户态时，<code>usertrapret()</code>
函数会被调用，<code>trapframe-&gt;kernel_trap = (uint64)usertrap;</code>
将 <code>usertrap</code> 函数地址存储在 <code>trapframe</code> 的偏移量
16 处，于是下次用户态发生 <code>trap</code> 时，<code>uservec</code>
就能正确跳转到 <code>usertrap</code> 函数。</p></li>
</ol></li>
</ol>
]]></content>
      <categories>
        <category>uCore</category>
      </categories>
      <tags>
        <tag>OS</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>github pages + cloudflare DNS 托管避坑</title>
    <url>/issue-resolve/49265.html</url>
    <content><![CDATA[<h2 id="问题描述">问题描述</h2>
<p>今天听同学说腾讯云的域名很便宜，十年才 230 CNY，非常心动，遂入手域名
<a href="https://0b1t.tech">0b1t.tech</a>，买完想把原本托管在 github
上的博客重定向到新的域名上的，没想到迁移出了问题。</p>
<p>一开始都很顺利，但是发现腾讯云自带的 dnspod 服务只能添加两条 DNS
记录，怎么能受这气？直接登上我的 cloudflare，把 DNS 服务改成
cloudflare。没想到就是 cloudflare DNS
默认打开的代理让我折腾了一个晚上。一开始没有发现这个问题，试了 A 记录和
CNAME 记录，结果一直不行，提示</p>
<blockquote>
<p>unavailable for your site because your domain is not properly
configured to support https</p>
</blockquote>
<p>这不是恶心人吗？https
打勾的框一直都是灰色的，无法勾选，受不了了，到网上去找相似的
issue，还真给我找到了解决方法。</p>
<h2 id="解决方案">解决方案</h2>
<p>社区很多讨论中都说要检查下面的几步：</p>
<blockquote>
<p>Check SSL Certificate Installation: Make sure that your SSL
certificate is properly installed for the domain. If you’re using a
certificate provider, ensure that the certificate is correctly
configured for your domain and is not expired.</p>
<p>Verify DNS Settings: Confirm that your domain’s DNS settings point to
the correct server where your site is hosted, and make sure the CNAME or
A records are set up correctly. You should also make sure that any DNS
propagation has fully completed. DNS changes can take some time (up to
48 hours), so double-check that your records are pointing to the right
place.</p>
<p>Check Server Configuration: Ensure that your server is configured to
accept HTTPS requests on port 443. Sometimes web servers require
specific configuration changes to allow HTTPS traffic. For example, in
Apache or Nginx, make sure the SSL configuration in your site’s server
block or virtual host is correctly set.</p>
<p>Redirect HTTP to HTTPS: Add HTTP to HTTPS redirects in your server
configuration. This ensures that users who try to visit your site using
HTTP will automatically be redirected to the secure HTTPS version of the
site.</p>
</blockquote>
<p>但是经过我实测，如果你跟着一般教程走的话上面这些问题都不会碰到的，直到我看到下面这个讨论贴：<a href="https://github.com/orgs/community/discussions/23049">How to enable
https support on custom domains
#23049</a>，可以看到有很多回复，我一个个翻下去，还真有人提到了
cloudflare proxy 的问题 <a href="https://github.com/orgs/community/discussions/23049#discussioncomment-3573123">“If
you’re using Cloudflare you need to disable their proxy feature”</a></p>
<p>确实啊，我还没试过关掉 cloudflare 的代理！直接切换到 cloudflare
控制台，把代理关掉，配置如下图 <img src="/issue-resolve/49265/1-DNS.png" class="" title="DNS"> 然后我使用<a href="https://www.nslookup.io/">在线 DNS 检查网站</a>来查看我域名的
DNS， <img src="/issue-resolve/49265/2-check.png" class="" title="check"> 这不就对味了吗！ 再把 github page
在设置页静置一下，没想到过一会儿它自己就跑完了，也能勾选 https
了，好耶！ <img src="/issue-resolve/49265/3-HTTPS.png" class="" title="https"></p>
]]></content>
      <categories>
        <category>issue resolve</category>
      </categories>
      <tags>
        <tag>domain configure</tag>
      </tags>
  </entry>
  <entry>
    <title>浅析 eBPF</title>
    <url>/kernel/50584.html</url>
    <content><![CDATA[<h2 id="ebpf-工作原理总览">eBPF 工作原理总览</h2>
<p>eBPF，一个源于 Linux
内核的革命性技术，允许我们在内核这个特权环境中安全地运行沙盒程序。值得一提的是，BPF
最初是 “伯克利数据包过滤器” 的缩写，但如今它的能力已远超网络领域，因此
“BPF” 已成为一个独立术语，而其前身则被称为 <strong>cBPF (classic
BPF)</strong>。</p>
<p>eBPF 程序并非直接在 CPU
上运行，而是在一个位于内核中的、高度安全和高效的 <strong>BPF
虚拟机</strong> 上执行。我们可以将该虚拟机理解为一个沙盒，确保 eBPF
代码不会导致 kernel panic。</p>
<img src="/kernel/50584/kernel_panic.jpg" class="" title="panic">
<p>为了让这个虚拟机正常运行，我们需要给它一套指令，这就是 <strong>BPF
指令集</strong>。我们用 C 语言等高级语言写好的逻辑，通过编译器（如
LLVM/Clang）翻译成 BPF 指令集字节码，然后才能交给虚拟机执行。</p>
<p>但光有代码执行是不够的，eBPF
程序经常需要和用户空间程序或者内核的其他部分交换数据，这时就需要用到
<strong>BPF Maps</strong>。这是一种高效的键值对存储，是 eBPF
程序与外界沟通的桥梁。</p>
<p>此外，为了让 eBPF 程序能与内核交互（比如获取当前进程
ID），内核提供了一组固定的<strong>辅助函数 (Helper
Functions)</strong>。eBPF
程序只能调用这些预设的、安全的函数，从而在获得强大能力的同时，保证了内核稳定。</p>
<p>最后，编译好的 BPF 字节码在通过 <code>bpf()</code>
系统调用加载到内核时，会经过一个严格的<strong>校验器 (Verifier)</strong>
检查，确保代码无害。通过后，字节码可以被<strong>即时编译器
(JIT)</strong> 翻译成原生机器码，以接近本地代码的速度执行。</p>
<img src="/kernel/50584/%E6%80%BB%E8%A7%88.png" class="" title="总览">
<p>接下来，我们将逐一拆解这些核心组件。</p>
<h2 id="bpf-虚拟机与指令集">BPF 虚拟机与指令集</h2>
<p>BPF
程序运行在一个小巧而高效的虚拟机上。这个虚拟机有自己的一套寄存器和指令集，就像一个微型的
CPU。</p>
<ul>
<li><strong>寄存器</strong>: BPF 虚拟机有 11 个 64 位寄存器。
<ul>
<li><code>R0</code> - <code>R9</code>:
通用寄存器，用于计算、存储数据和传递参数。</li>
<li><code>R10</code>: 只读的帧指针寄存器，用于访问栈空间。</li>
</ul></li>
</ul>
<img src="/kernel/50584/%E8%99%9A%E6%8B%9F%E6%9C%BA.png" class="" title="vm">
<ul>
<li><strong>指令集</strong>: BPF
指令集中，所有指令都是固定长度的，运行更加简单高效。</li>
</ul>
<h3 id="指令格式">指令格式</h3>
<p>BPF 指令集官方说明可以参考 Linux 内核文档：<a href="https://web.git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/bpf/standardization/instruction-set.rst">BPF
Instruction Set</a></p>
<p>BPF 指令有两种格式：</p>
<ol type="1">
<li><strong>基本指令 (64-bit)</strong>:
这是最常见的格式，由操作码、寄存器、偏移量和立即数组成。所有未使用的字段都必须为零。
<ul>
<li><code>opcode</code>: 8 bit (操作码)</li>
<li><code>regs</code>: 8 bit (目标寄存器: 4 bit, 源寄存器: 4
bit)，在大端序和小端序主机上，这两个 4-bit 字段的顺序是相反的。</li>
<li><code>offset</code>: 16 bit (偏移量)</li>
<li><code>imm</code>: 32 bit (立即数)</li>
</ul></li>
<li><strong> 宽指令 (128-bit)</strong>: 专用于加载一个 64
位的立即数（ld_imm64）。它由两个连续的 64 位指令构成。
<ul>
<li>第一个 64 位指令：正常的基本指令格式，其中 imm 字段存放 64
位立即数的低 32 位</li>
<li>第二个 64 位指令：前 32 位（opcode + regs + offset）必须全部为
0，imm 字段存放 64 位立即数的高 32 位</li>
</ul></li>
</ol>
<h3 id="操作码-opcode">操作码 (Opcode)</h3>
<p><code>opcode</code> 决定了一条指令做什么。它的最后 3
位定义了指令的<strong>类别 (Class)</strong>。</p>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">value</th>
<th style="text-align: left;">class</th>
<th style="text-align: left;"> 描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> 0x00</td>
<td style="text-align: left;">LD</td>
<td style="text-align: left;"><strong> 加载
(Load)</strong>，非标准加载操作，主要用于 <code>ld_imm64</code>
宽指令。</td>
</tr>
<tr>
<td style="text-align: left;">0x01</td>
<td style="text-align: left;">LDX</td>
<td style="text-align: left;"><strong> 加载
(Load)</strong>，从内存读取数据到寄存器。</td>
</tr>
<tr>
<td style="text-align: left;">0x02</td>
<td style="text-align: left;">ST</td>
<td style="text-align: left;"><strong> 存储
(Store)</strong>，将立即数写入内存。</td>
</tr>
<tr>
<td style="text-align: left;">0x03</td>
<td style="text-align: left;">STX</td>
<td style="text-align: left;"><strong> 存储
(Store)</strong>，将寄存器的值写入内存。</td>
</tr>
<tr>
<td style="text-align: left;">0x04</td>
<td style="text-align: left;">ALU</td>
<td style="text-align: left;">32 位算术与逻辑运算。</td>
</tr>
<tr>
<td style="text-align: left;">0x05</td>
<td style="text-align: left;">JMP</td>
<td style="text-align: left;"><strong> 跳转
(Jump)</strong>，64 位操作数的条件或无条件跳转。</td>
</tr>
<tr>
<td style="text-align: left;">0x06</td>
<td style="text-align: left;">JMP32</td>
<td style="text-align: left;"><strong> 跳转
(Jump)</strong>，32 位操作数的条件或无条件跳转。</td>
</tr>
<tr>
<td style="text-align: left;">0x07</td>
<td style="text-align: left;">ALU64</td>
<td style="text-align: left;">64 位算术与逻辑运算。</td>
</tr>
</tbody>
</table>
<h3 id="存取指令-ldldx-ststx">存取指令 (LD/LDX, ST/STX)</h3>
<p>这类指令负责在寄存器和内存之间搬运数据。它的 <code>opcode</code>
除了类别外，还包含了<strong>寻址模式 (Mode)</strong> 和<strong>数据大小
(Size)</strong>。</p>
<p><strong>寻址模式 (Mode)</strong></p>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">value</th>
<th style="text-align: left;">mode modifier</th>
<th style="text-align: left;"> 描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> 0</td>
<td style="text-align: left;">IMM</td>
<td style="text-align: left;"><strong> 立即数 (Immediate)</strong>，用于
<code>ld_imm64</code> 指令加载 64 位立即数。</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">ABS</td>
<td style="text-align: left;"><strong> 绝对寻址
(Absolute)</strong>，用于传统 cBPF，已废弃。</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">IND</td>
<td style="text-align: left;"><strong> 间接寻址
(Indirect)</strong>，用于传统 cBPF，已废弃。</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">MEM</td>
<td style="text-align: left;"><strong> 常规内存操作
(Memory)</strong>，最常见的加载和存储模式。</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">MEMSX</td>
<td style="text-align: left;"><strong> 符号扩展加载 (Sign-extending
load)</strong>，加载时将有符号小整数正确扩展到 64 位。</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">ATOMIC</td>
<td style="text-align: left;"><strong> 原子操作
(Atomic)</strong>，用于实现安全的并发数据修改。</td>
</tr>
</tbody>
</table>
<p><strong>数据大小 (Size)</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">value</th>
<th style="text-align: left;">size</th>
<th style="text-align: left;"> 描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> 0</td>
<td style="text-align: left;">W</td>
<td style="text-align: left;"><strong> 字 (Word)</strong>, 4 字节</td>
</tr>
<tr>
<td style="text-align: left;"> 1</td>
<td style="text-align: left;">H</td>
<td style="text-align: left;"><strong> 半字 (Half-word)</strong>, 2
字节</td>
</tr>
<tr>
<td style="text-align: left;"> 2</td>
<td style="text-align: left;">B</td>
<td style="text-align: left;"><strong> 字节 (Byte)</strong>, 1 字节</td>
</tr>
<tr>
<td style="text-align: left;"> 3</td>
<td style="text-align: left;">DW</td>
<td style="text-align: left;"><strong> 双字 (Double-word)</strong>, 8
字节</td>
</tr>
</tbody>
</table>
<h4 id="位立即数加载-ld_imm64">64 位立即数加载 (ld_imm64)</h4>
<p><code>ld_imm64</code> (Class <code>0x00</code>, Mode
<code>0x00</code>, Size <code>0x03</code>) 是一条非常特殊的指令，使用
128 位的宽格式。它不仅能加载一个 64 位的常量，还能通过
<code>src_reg</code> 字段的不同取值，实现加载 Map 地址等高级功能，是连接
BPF 程序和外部资源的关键。</p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><code>src_reg</code></th>
<th style="text-align: left;">伪代码</th>
<th style="text-align: left;"><code>imm</code> 类型</th>
<th style="text-align: left;">目标寄存器类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> 0x0</td>
<td style="text-align: left;"><code>dst = (next_imm &lt;&lt; 32) \| imm</code></td>
<td style="text-align: left;">整数</td>
<td style="text-align: left;">整数</td>
</tr>
<tr>
<td style="text-align: left;"> 0x1</td>
<td style="text-align: left;"><code>dst = map_by_fd(imm)</code></td>
<td style="text-align: left;">Map 文件描述符</td>
<td style="text-align: left;"> Map 地址</td>
</tr>
<tr>
<td style="text-align: left;"> 0x2</td>
<td style="text-align: left;"><code>dst = map_val(map_by_fd(imm)) + next_imm</code></td>
<td style="text-align: left;">Map 文件描述符</td>
<td style="text-align: left;"> Map 值地址</td>
</tr>
<tr>
<td style="text-align: left;"> 0x5</td>
<td style="text-align: left;"><code>dst = map_by_idx(imm)</code></td>
<td style="text-align: left;">Map 程序内索引</td>
<td style="text-align: left;"> Map 地址</td>
</tr>
<tr>
<td style="text-align: left;"> 0x6</td>
<td style="text-align: left;"><code>dst = map_val(map_by_idx(imm)) + next_imm</code></td>
<td style="text-align: left;">Map 程序内索引</td>
<td style="text-align: left;"> Map 值地址</td>
</tr>
</tbody>
</table>
<p>上面的表格就解释了在 BPF C 代码中，表面上直接引用的 Map
变量，在底层是如何通过 <code>ld_imm64</code>
指令被解析成实际内存地址的。</p>
<h4 id="常规与符号扩展加载存储">常规与符号扩展加载 / 存储</h4>
<ul>
<li><strong>常规加载 / 存储 (<code>MEM</code>)</strong>:
<ul>
<li><code>*(size *) (dst_reg + offset) = src_reg</code> (STX)</li>
<li><code>*(size *) (dst_reg + offset) = imm</code> (ST)</li>
<li><code>dst_reg = *(unsigned size *) (src_reg + offset)</code>
(LDX)</li>
</ul></li>
<li><strong> 符号扩展加载 (<code>MEMSX</code>)</strong>:
<ul>
<li><code>dst_reg = *(signed size *) (src_reg + offset)</code>
(LDX)</li>
<li> 用于从内存加载一个有符号的小整数（如 <code>s8</code>,
<code>s16</code>）到 64
位寄存器时，能正确地将符号位扩展到高位，保持数值不变。</li>
</ul></li>
</ul>
<h4 id="原子操作-atomic">原子操作 (Atomic)</h4>
<p>原子操作通过 <code>STX</code> 指令实现，使用 <code>ATOMIC</code>
寻址模式，支持 32 位 (<code>W</code>) 和 64 位 (<code>DW</code>)
操作。<code>imm</code> 字段用于编码具体的操作。</p>
<ul>
<li><strong>简单原子操作</strong>: <code>imm</code>
字段的值与算术指令类似。
<ul>
<li><code>ADD (0x00)</code>:
<code>*(size *)(dst + offset) += src</code></li>
<li><code>OR (0x40)</code>, <code>AND (0x50)</code>,
<code>XOR (0xa0)</code></li>
</ul></li>
<li><strong>复杂原子操作</strong>:
<ul>
<li><code>XCHG (0xe1)</code>: 原子地交换 <code>src</code> 和
<code>*(dst + offset)</code> 的值。</li>
<li><code>CMPXCHG (0xf1)</code>: 原子地比较 <code>*(dst + offset)</code>
和 <code>R0</code>，如果相等，则将 <code>src</code> 写入
<code>*(dst + offset)</code>。无论如何，都将内存旧值加载回
<code>R0</code>。</li>
</ul></li>
<li><strong>FETCH 修饰符</strong>: 如果 <code>imm</code> 中包含
<code>FETCH (0x01)</code>
标志，操作会在执行后，将内存中的<strong>旧值</strong>返回到
<code>src_reg</code>。<code>XCHG</code> 和 <code>CMPXCHG</code>
总是隐式包含 <code>FETCH</code>。</li>
</ul>
<h3 id="算术与跳转指令-alujmp">算术与跳转指令 (ALU/JMP)</h3>
<p>这类指令的 <code>opcode</code> 除了类别，还包含<strong>操作码
(Code)</strong> 和<strong>源 (Source)</strong>。</p>
<p><strong>操作数来源 (Source)</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">value</th>
<th style="text-align: left;">source</th>
<th style="text-align: left;"> 描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> 0</td>
<td style="text-align: left;">K</td>
<td style="text-align: left;"> 操作数是一个 32 位立即数
<code>imm</code>。</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"> 操作数是另一个寄存器
<code>src_reg</code>。</td>
</tr>
</tbody>
</table>
<h4 id="算术指令-alualu64">算术指令 (ALU/ALU64)</h4>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">name</th>
<th style="text-align: left;">code</th>
<th style="text-align: left;"> 描述与边界情况</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> ADD/SUB</td>
<td style="text-align: left;">0x0/0x1</td>
<td style="text-align: left;"> 加 / 减法，允许溢出回绕。</td>
</tr>
<tr>
<td style="text-align: left;">MUL/DIV</td>
<td style="text-align: left;">0x2/0x3</td>
<td style="text-align: left;"> 乘 / 除法。<strong>除以零时，结果为
0</strong>。有符号除法 <code>SDIV</code> 中，<code>LLONG_MIN / -1</code>
结果仍为 <code>LLONG_MIN</code>。</td>
</tr>
<tr>
<td style="text-align: left;">MOD</td>
<td style="text-align: left;">0x9</td>
<td style="text-align: left;"> 取模。<strong>对零取模时，ALU64 下 dst
不变，ALU32 下 dst 高 32 位清零</strong>。有符号取模 <code>SMOD</code>
中，<code>LLONG_MIN % -1</code> 结果为 0。</td>
</tr>
<tr>
<td style="text-align: left;">OR/AND/XOR</td>
<td style="text-align: left;">0x4/0x5/0xa</td>
<td style="text-align: left;"> 按位或 / 与 / 异或。</td>
</tr>
<tr>
<td style="text-align: left;">LSH/RSH</td>
<td style="text-align: left;">0x6/0x7</td>
<td style="text-align: left;"> 逻辑左 / 右移。移动位数会被
<code>&amp; 63</code> (64 位) 或 <code>&amp; 31</code> (32 位) 屏蔽。</td>
</tr>
<tr>
<td style="text-align: left;">ARSH</td>
<td style="text-align: left;">0xc</td>
<td style="text-align: left;"> 算术右移（带符号扩展）。</td>
</tr>
<tr>
<td style="text-align: left;">NEG</td>
<td style="text-align: left;">0x8</td>
<td style="text-align: left;"> 取负。</td>
</tr>
<tr>
<td style="text-align: left;">MOV</td>
<td style="text-align: left;">0xb</td>
<td style="text-align: left;"> 移动。</td>
</tr>
<tr>
<td style="text-align: left;">MOVSX</td>
<td style="text-align: left;">0xb</td>
<td style="text-align: left;"><strong> 符号扩展移动</strong>。从
<code>src</code> 移动 <code>s8</code>, <code>s16</code> 或
<code>s32</code> 到 <code>dst</code> 并进行符号扩展。</td>
</tr>
<tr>
<td style="text-align: left;">END</td>
<td style="text-align: left;">0xd</td>
<td style="text-align: left;"><strong> 字节序转换</strong>。<code>ALU</code>
模式下，<code>source</code> 位决定是转为小端 (<code>LE</code>) 还是大端
(<code>BE</code>)。<code>ALU64</code> 模式下，进行无条件的大小端翻转
(<code>bswap</code>)。<code>imm</code> 决定宽度 (16/32/64)。</td>
</tr>
</tbody>
</table>
<h4 id="跳转指令-jmpjmp32">跳转指令 (JMP/JMP32)</h4>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">code</th>
<th style="text-align: left;">value</th>
<th style="text-align: left;"> 描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"> JA</td>
<td style="text-align: left;">0x0</td>
<td style="text-align: left;"> 无条件跳转。<code>JMP</code> 版用
<code>offset</code> (16 位)，<code>JMP32</code> 版用 <code>imm</code>
(32 位)。</td>
</tr>
<tr>
<td style="text-align: left;">JEQ/JNE</td>
<td style="text-align: left;">0x1/0x5</td>
<td style="text-align: left;"> 等于 / 不等于则跳转。</td>
</tr>
<tr>
<td style="text-align: left;">JGT/JGE</td>
<td style="text-align: left;">0x2/0x3</td>
<td style="text-align: left;"><strong> 无符号</strong>大于 / 大于等于则跳转。</td>
</tr>
<tr>
<td style="text-align: left;">JLT/JLE</td>
<td style="text-align: left;">0xa/0xb</td>
<td style="text-align: left;"><strong> 无符号</strong>小于 / 小于等于则跳转。</td>
</tr>
<tr>
<td style="text-align: left;">JSGT/JSGE</td>
<td style="text-align: left;">0x6/0x7</td>
<td style="text-align: left;"><strong> 有符号</strong>大于 / 大于等于则跳转。</td>
</tr>
<tr>
<td style="text-align: left;">JSLT/JSLE</td>
<td style="text-align: left;">0xc/0xd</td>
<td style="text-align: left;"><strong> 有符号</strong>小于 / 小于等于则跳转。</td>
</tr>
<tr>
<td style="text-align: left;">JSET</td>
<td style="text-align: left;">0x4</td>
<td style="text-align: left;"> 按位与结果非零则跳转。</td>
</tr>
<tr>
<td style="text-align: left;">CALL</td>
<td style="text-align: left;">0x8</td>
<td style="text-align: left;"><strong> 函数调用</strong>。这是 eBPF
能力扩展的核心。</td>
</tr>
<tr>
<td style="text-align: left;">EXIT</td>
<td style="text-align: left;">0x9</td>
<td style="text-align: left;"><strong> 程序返回</strong>，终止执行。在子函数中调用则返回到调用点。</td>
</tr>
</tbody>
</table>
<p><code>CALL</code> 指令有几种不同的用法，通过 <code>src_reg</code>
区分：</p>
<ul>
<li><code>src_reg = 0</code>: 调用由 <code>imm</code> 指定 ID
的<strong>内核辅助函数</strong>。</li>
<li><code>src_reg = 1</code>:
调用<strong>程序内本地函数</strong>，<code>imm</code>
是相对于当前指令的偏移量。</li>
<li><code>src_reg = 2</code>: 通过 <code>imm</code> 指定的 <strong>BTF
ID</strong> 来调用内核辅助函数。</li>
</ul>
<h2 id="核心组件bpf-maps">核心组件：BPF Maps</h2>
<p>如果说 BPF 程序是处理数据的 “工人”，那 <strong>BPF Maps</strong>
就是工人们存放工具、交换半成品的 “仓库”。前面我们已经看到，<code>ld_imm64</code>
指令是如何在底层将 Map 翻译成地址的。</p>
<p>BPF Maps
是一种通用的、存在于内核中的键值对存储。它有以下几个关键作用：</p>
<ol type="1">
<li><strong>状态保持</strong>: BPF
程序本身是无状态的，一次事件触发执行完所有信息就丢失了。Maps
可以在多次执行之间保存状态，比如，用来统计某个网络包出现的次数。</li>
<li><strong>内外通信</strong>: 用户空间的控制程序可以通过读写 Maps，向
BPF 程序传递配置，或者从 BPF 程序中获取采集到的数据。</li>
<li><strong>程序间通信</strong>: 一个 BPF 程序可以将数据写入 Map，另一个
BPF 程序可以从中读取，实现程序间的协作。</li>
</ol>
<p>内核提供了多种 Map 类型，以适应不同场景，例如：</p>
<ul>
<li><code>BPF_MAP_TYPE_HASH</code>: 高效哈希表。</li>
<li><code>BPF_MAP_TYPE_ARRAY</code>: 高效数组。</li>
<li><code>BPF_MAP_TYPE_PERF_EVENT_ARRAY</code>:
用于向用户空间发送性能事件数据。</li>
<li><code>BPF_MAP_TYPE_RINGBUF</code>:
高性能的环形缓冲区，向用户空间传递数据。</li>
<li><code>BPF_MAP_TYPE_LRU_HASH</code>: 带 LRU
淘汰的哈希表，适合连接追踪。<br>
</li>
<li><code>BPF_MAP_TYPE_ARRAY_OF_MAPS</code> /
<code>BPF_MAP_TYPE_HASH_OF_MAPS</code>: Map 嵌套，用于多 namespace
或多租户隔离。</li>
</ul>
<h2 id="能力扩展内核辅助函数">能力扩展：内核辅助函数</h2>
<p>为了安全，BPF
虚拟机是一个完全隔离的沙盒，它不能随意调用内核里的任意函数。那它如何与内核交互呢？答案是
<strong>内核辅助函数 (Helper Functions)</strong>。</p>
<p>正如我们在 <code>CALL</code> 指令中看到的，BPF
程序可以通过特定指令调用一份内核提供的 “白名单” 函数。这些函数就是辅助函数，它们是
eBPF 程序能力的源泉，提供了诸如：</p>
<ul>
<li><strong>访问 Maps</strong>:
<code>bpf_map_lookup_elem()</code>、<code>bpf_map_update_elem()</code></li>
<li><strong>获取上下文信息</strong>:
<code>bpf_get_current_pid_tgid()</code>、<code>bpf_ktime_get_ns()</code></li>
<li><strong>网络包处理</strong>: <code>bpf_skb_store_bytes()</code></li>
<li><strong>打印调试</strong>: <code>bpf_printk()</code></li>
</ul>
<p>eBPF
程序只能调用这个固定的函数集合，这既赋予了它强大的能力，也从根本上保证了内核的安全。</p>
<h2 id="bpf-程序类型">BPF 程序类型</h2>
<p>eBPF
程序不是随便挂在内核里就能运行的，它必须附着在内核的特定<strong>挂载点
(Hook Point)</strong> 上。<strong>BPF 程序类型 (Program Type)</strong>
就定义了程序可以挂载在哪里，以及它能做什么。</p>
<p>不同的程序类型，决定了 eBPF 的应用场景。例如：</p>
<ul>
<li><strong>Kprobes / Tracepoints</strong>:
用于内核态函数追踪和性能分析，是可观测性工具（如
<code>bcc</code>、<code>bpftrace</code>）的根本。</li>
<li><strong>XDP (eXpress Data Path)</strong>:
挂载在网卡驱动层，能在网络包进入协议栈前进行处理，实现超高性能的防火墙、负载均衡等。</li>
<li><strong>TC (Traffic Control)</strong>:
挂载在内核网络协议栈的队列规则上，用于实现复杂的网络包过滤和流量整形。</li>
<li><strong>LSM (Linux Security Modules)</strong>:
用于实现更灵活、更细粒度的安全策略。</li>
</ul>
<p>程序类型决定了 BPF 程序被调用时，<code>R1</code>
寄存器中传入的<strong>上下文 (Context)</strong>
类型（比如对于网络程序是网络包
<code>sk_buff</code>），以及它能调用的辅助函数集合。</p>
<h2 id="编译加载与校验">编译、加载与校验</h2>
<p>正常来说通常使用 C 语言的一个子集来编写 BPF 程序，然后用
<code>LLVM/Clang</code> 将其编译成 BPF 字节码。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">clang -O2 -target bpf -c bpf_program.c -o bpf.o</span><br></pre></td></tr></tbody></table></figure>
<p>编译出的字节码需要通过 <code>bpf()</code>
系统调用加载到内核中。现代开发通常使用 <code>libbpf</code>
这样的库来简化加载、挂载和与 Maps 交互的过程。</p>
<p>在程序运行前，内核的<strong>校验器 (Verifier)</strong>
会对字节码进行极其严格的静态分析，确保其安全性。校验器是 eBPF
安全模型的基石，它会检查：</p>
<ol type="1">
<li><strong>程序必须能终止</strong>:
禁止无限循环，通过有向无环图检查确保程序不会永远执行下去。</li>
<li><strong>内存访问安全</strong>:
确保不会访问越界的内存地址，不会读取未初始化的栈内存或寄存器。</li>
<li><strong>类型安全</strong>:
检查函数调用的参数类型是否正确，对上下文的访问是否合规。</li>
</ol>
<p>只有通过校验的 BPF 程序才被认为是安全的，并被允许加载到内核中。</p>
<details>
<summary>
Verifier 常见拒绝示例
</summary>
<table>
<thead>
<tr>
<th>报错片段</th>
<th>触发原因</th>
</tr>
</thead>
<tbody>
<tr>
<td> invalid indirect read from stack</td>
<td> 读到未初始化栈空间</td>
</tr>
<tr>
<td> possible pointer arithmetic on ctx</td>
<td> 对只读 ctx 指针做算术</td>
</tr>
<tr>
<td> jump out of range</td>
<td> 分支过深 /offset 溢出</td>
</tr>
</tbody>
</table>
</details>
<h3 id="co-recompile-once-run-everywhere">CO-RE（Compile Once – Run
Everywhere）</h3>
<p>如果目标机器内核启用了 <a href="https://www.kernel.org/doc/html/latest/bpf/btf.html">BTF</a>，建议在编译时打开
CO-RE：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">clang -g -O2 -target bpf -D__TARGET_ARCH_$(<span class="built_in">uname</span> -m) \</span><br><span class="line">      -c prog.c -o prog.o</span><br></pre></td></tr></tbody></table></figure>
<p>同一个 <code>.o</code> 文件可跨 5.x/6.x
内核直接重定位字段，无需重编译。</p>
<h2 id="执行解释器与-jit-编译器">执行：解释器与 JIT 编译器</h2>
<p>通过校验后，BPF 字节码终于可以执行了。内核提供了两种执行方式：</p>
<h3 id="解释器-interpreter">解释器 (Interpreter)</h3>
<p>解释器会逐条读取 BPF
字节码并模拟执行。它非常安全，但效率相对较低，因为每条指令都需要软件模拟。</p>
<h3 id="jit-编译器-just-in-time">JIT 编译器 (Just-In-Time)</h3>
<p>为了追求极致性能，eBPF 支持<strong>即时编译
(JIT)</strong>。在程序加载时，JIT 编译器会将整个 BPF
字节码动态地翻译成目标机器（如 x86-64, ARM64）的原生指令。</p>
<p>这样，当事件触发 BPF 程序时，CPU
可以直接执行这些编译好的原生指令，省去了逐条解释的开销，从而大大提高了执行效率，性能几乎与原生内核模块无异。</p>
<p>每个支持的 CPU 架构都有自己的 JIT 编译器实现，例如内核源码中的
<code>bpf_jit_comp.c</code>。</p>
<h2 id="运行-demo">运行 Demo</h2>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/libbpf/libbpf-bootstrap.git</span><br><span class="line"><span class="built_in">cd</span> libbpf-bootstrap/examples/c/minimal</span><br><span class="line">make minimal</span><br><span class="line"><span class="built_in">sudo</span> ./minimal</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">cat</span> /sys/kernel/debug/tracing/trace_pipe</span><br><span class="line">           &lt;...&gt;-3840345 [010] d... 3220701.101143: bpf_trace_printk: BPF triggered from PID 3840345.</span><br><span class="line">           &lt;...&gt;-3840345 [010] d... 3220702.101265: bpf_trace_printk: BPF triggered from PID 3840345.</span><br></pre></td></tr></tbody></table></figure>
<p>详细使用方法可以查看项目的 README 文件。</p>
<blockquote>
<p><strong>Takeaway</strong>: eBPF = 事件驱动 + 沙盒字节码 + Map +
Helper + Verifier + JIT</p>
</blockquote>
]]></content>
      <categories>
        <category>kernel</category>
      </categories>
      <tags>
        <tag>ebpf</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始的神经网络学习 (一)：浅析神经网络</title>
    <url>/Machine-learning/36604.html</url>
    <content><![CDATA[<h2 id="神经网络的概念">神经网络的概念</h2>
<p>想象一下，我们想教计算机识别图片中的猫，但是我们不能为它编写一套硬性的规则（“如果它有尖耳朵、胡须和毛茸茸的尾巴，那它就是一只猫”），因为现实世界中的猫品种各异，<del>而且猫娘也符合上面的规则</del>。</p>
<p>神经网络采用了一种不同的方法：<strong>从样本中学习</strong>。就像我们通过看到真实的各种猫之后才知道这种动物叫 “猫” 一样，神经网络也会处理成千上万张标记好的图片（“这是猫”，“这不是猫”），并通过这种方法逐渐 “学会” 识别猫的视觉模式。</p>
<p>从技术上讲，神经网络是一种受人脑结构启发的计算模型。它由许多简单的处理单元 —— <strong>神经元
(Neuron)</strong> 组成，这些神经元组织在不同的<strong>层
(Layer)</strong> 中。</p>
<p>一个经典的神经网络有三层：</p>
<ul>
<li><strong>输入层 (Input Layer)</strong>:
负责接收最原始的数据。例如，对于一张图片，每个神经元可能对应图片中的一个像素值。</li>
<li><strong>隐藏层 (Hidden Layers)</strong>:
位于输入层和输出层之间。这些是网络进行大部分 “思考” 的地方。一个神经网络可以没有隐藏层，也可以有很多个。层数越多，网络通常越 “深”。</li>
<li><strong>输出层 (Output Layer)</strong>:
产生最终的结果。例如，在猫识别任务中，输出层可能只有一个神经元，其输出值在 0 到 1 之间，表示图片是猫的概率。</li>
</ul>
<img src="/Machine-learning/36604/1-NN.png" class="" title="NN">
<h2 id="前向传播">前向传播</h2>
<p>数据在网络中流动并最终得到一个预测结果的过程就被称为<strong>前向传播
(Forward Propagation)</strong>。</p>
<h3 id="神经元和激活值">神经元和激活值</h3>
<p>简单来说，神经元就是持有一个数的简单单元，这个数称为<strong>激活值
(Activation)</strong>。输入层的神经元激活值就是我们输入的数据本身。对于其他层的神经元，它的激活值需要通过计算得出。</p>
<h3 id="连接和权重">连接和权重</h3>
<p>不同层的神经元之间通过连接进行通信。每个连接都有一个<strong>权重
(Weight)</strong>，这个数代表了连接的强度和重要性。</p>
<ul>
<li>一个较大的正权重意味着前一个神经元的激活会对后一个神经元产生强烈的 “兴奋” 作用。</li>
<li>一个较大的负权重则相反，表示会产生强烈的 “抑制” 作用。</li>
<li>接近于零的权重意味着前一个神经元对后一个神经元几乎没有影响。</li>
</ul>
<p><strong>训练神经网络的本质，就是调整这些权重的值。</strong></p>
<img src="/Machine-learning/36604/3-weights.png" class="" title="weights">
<h3 id="计算激活值">计算激活值</h3>
<p>一个神经元激活值的计算分两步：</p>
<ul>
<li><p><strong>计算加权和 (Weighted Sum)</strong>:
神经元接收来自前一层所有神经元的输入。它将每个输入的激活值乘以它们之间连接的权重，然后将所有结果相加。</p>
<img src="/Machine-learning/36604/4-weightedSum.png" class="" title="wSum">
<p>在加权和以外，我们还会加上一个额外的数字，叫做<strong>偏置
(Bias)</strong>。偏置的作用是提供一个可调的 “基础激活水平”。你可以把它看作是，在没有任何输入的情况下，一个神经元有多容易被激活。比如，一个高的偏置使得神经元更容易被激活。</p>
<p>所以，一个神经元的 “预激活值”（我们称之为
<code>z</code>）的完整公式是：</p>
<p><span class="math display">$$ z = \sum_{i=1}^{n}(w_i a_i) + bias
$$</span></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">inputs = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">weights = [<span class="number">0.2</span>, <span class="number">0.8</span>, -<span class="number">0.5</span>]</span><br><span class="line">bias = <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">z = inputs[<span class="number">0</span>] * weights[<span class="number">0</span>] + inputs[<span class="number">1</span>] * weights[<span class="number">1</span>] + inputs[<span class="number">2</span>] * weights[<span class="number">2</span>] + bias</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="comment"># z = 2.3</span></span><br></pre></td></tr></tbody></table></figure>
<img src="/Machine-learning/36604/5-weightsAndBias.png" class="" title="wBias"></li>
<li><p><strong>应用激活函数</strong>:
如果我们只用加权和，那么无论网络有多少层，它本质上都只是在做一个简单的线性变换。这限制了它学习复杂模式的能力。</p>
<p>为了引入<strong>非线性
(non-linearity)</strong>，我们需要一个<strong>激活函数</strong>。它接收上一步计算出的
<code>z</code> 值，并输出最终的激活值 <code>a</code>。</p>
<p>常见的激活函数有很多，这里我们介绍 <strong>ReLU (Rectified Linear
Unit，修正线性单元)</strong>。它的规则非常简单：</p>
<ul>
<li>如果输入 <code>z</code> 大于 0，输出就是 <code>z</code> 本身。</li>
<li>如果输入 <code>z</code> 小于或等于 0，输出就是 0。</li>
</ul>
<p>其数学表达式为：</p>
<p><span class="math display"><em>f</em>(<em>z</em>) = max (0, <em>z</em>)</span></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">activation = relu(<span class="number">2.3</span>)</span><br><span class="line">activation_neg = relu(-<span class="number">1.2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"ReLU(2.3) = <span class="subst">{activation}</span>, ReLU(-1.2) = <span class="subst">{activation_neg}</span>"</span>)</span><br><span class="line"><span class="comment"># ReLU(2.3) = 2.3, ReLU(-1.2) = 0.0</span></span><br></pre></td></tr></tbody></table></figure>
<p>ReLU
之所以受欢迎，是因为它在计算上很高效，并且在实践中能帮助网络更有效地学习。</p></li>
</ul>
<h3 id="矩阵运算">矩阵运算</h3>
<p>当神经网络变得很大时，逐个计算每个神经元的加权和会非常慢。幸运的是，我们可以使用线性代数中的矩阵运算来一次性完成一整层神经元的计算。</p>
<blockquote>
<p>注：下文所有涉及的 <code>@</code> 运算符为 Python 3.5+
的矩阵乘法（matmul），推荐用于神经网络等线性代数场景。<code>np.dot</code>
在一维时是内积，二维时是矩阵乘法，但 <code>@</code>
始终表示矩阵乘法，更直观。</p>
</blockquote>
<p>我们可以把：</p>
<ul>
<li>一层的输入激活值看作一个向量 <code>A</code>。</li>
<li>连接到下一层的所有权重组织成一个矩阵 <code>W</code>。</li>
<li>所有偏置组成一个向量 <code>B</code>。</li>
</ul>
<p>那么，下一层所有神经元的预激活值 <code>Z</code>
就可以通过一个简单的公式计算出来：</p>
<p><code>Z = A @ W + B</code></p>
<p>其中 <code>@</code> 代表矩阵乘法。</p>
<p>假设我们有一个包含 2 个样本的批次（batch），每个样本有 3 个特征（输入），我们要将其传入一个有 4 个神经元的隐藏层。</p>
<ul>
<li><code>A</code> (激活值矩阵) 的形状是 <code>(2, 3)</code></li>
<li><code>W</code> (权重矩阵) 的形状是 <code>(3, 4)</code></li>
<li><code>B</code> (偏置向量) 的形状是 <code>(1, 4)</code> (它会被广播到
<code>(2, 4)</code>)</li>
<li><code>Z</code> (输出预激活矩阵) 的形状将是 <code>(2, 4)</code></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">activations = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">weights = np.random.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"权重: "</span>, weights)</span><br><span class="line">biases = np.random.rand(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"偏置: "</span>, biases)</span><br><span class="line"></span><br><span class="line">Z = activations @ weights + biases</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Z 的形状:"</span>, Z.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Z 的值:\n"</span>, Z)</span><br><span class="line"><span class="comment"># 权重:  [[0.08746301 0.34148947 0.2321176  0.49574324]</span></span><br><span class="line"><span class="comment">#  [0.69313075 0.77251665 0.48220435 0.38541945]</span></span><br><span class="line"><span class="comment">#  [0.48551131 0.84943177 0.05167356 0.03890675]]</span></span><br><span class="line"><span class="comment"># 偏置:  [[0.05122642 0.2860902  0.75485931 0.75830747]]</span></span><br><span class="line"><span class="comment"># Z 的形状: (2, 4)</span></span><br><span class="line"><span class="comment"># Z 的值:</span></span><br><span class="line"><span class="comment">#  [[ 2.98148486  4.72090828  2.1064063   2.14160985]</span></span><br><span class="line"><span class="comment">#  [ 6.77980008 10.61122193  4.40439285  4.90181816]]</span></span><br></pre></td></tr></tbody></table></figure>
<p>这正是现代深度学习框架 (如 TensorFlow 和
PyTorch) 在底层所做的事情，它能极大地利用 GPU 的并行计算能力。</p>
<h4 id="深入理解偏置和广播-a-deeper-look-at-bias-and-broadcasting">深入理解偏置和广播
(A Deeper Look at Bias and Broadcasting)</h4>
<p>在上面 <a href="#矩阵运算">2.4</a> 节中，你可能会问：“为什么偏置
<code>B</code> 的形状是 <code>(1, 4)</code>，却能和形状为
<code>(2, 4)</code> 的矩阵 <code>A · W</code> 相加呢？”</p>
<p>这要归功于 NumPy 的一个强大特性：<strong>广播
(Broadcasting)</strong>。</p>
<p>我们的隐藏层有 4 个神经元，所以我们有 4 个对应的偏置值（存储在
<code>B</code> 中）。当我们一次性处理一个包含 2 个样本的批次
(batch) 时，这 4 个偏置值需要被分别应用到<strong>每一个</strong>样本的计算结果上。当
NumPy 看到你要将一个 <code>(2, 4)</code> 矩阵和一个 <code>(1, 4)</code>
向量相加时，它会自动将这个 <code>(1, 4)</code>
的行向量 “拉伸” 或 “复制” 成一个 <code>(2, 4)</code>
的矩阵，使其形状匹配，然后再执行元素间的加法。NumPy
并不会真的创建这个大矩阵的副本，而是通过一种更巧妙的内部机制来完成计算，节省了大量内存和计算时间。</p>
<p>简单来说，广播机制让我们能用简洁的代码，将同一组偏置高效地应用到一批数据中的所有样本上。</p>
<p>我们可以通过一个具体的代码示例来观察广播是如何工作的:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A_dot_W = np.array([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>], [<span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>]])</span><br><span class="line"></span><br><span class="line">B = np.array([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"A · W 的形状:"</span>, A_dot_W.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"B 的形状:"</span>, B.shape)</span><br><span class="line"></span><br><span class="line">Z = A_dot_W + B</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n--- 执行 A_dot_W + B ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"在广播后，B 表现得像下面这个 (2, 4) 的矩阵:"</span>)</span><br><span class="line"></span><br><span class="line">broadcasted_B_for_demo = np.tile(B, (<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(broadcasted_B_for_demo)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n最终结果 Z 的形状:"</span>, Z.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"最终结果 Z 的值:\n"</span>, Z)</span><br><span class="line"><span class="comment"># A · W 的形状: (2, 4)</span></span><br><span class="line"><span class="comment"># B 的形状: (1, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 执行 A_dot_W + B ---</span></span><br><span class="line"><span class="comment"># 在广播后，B 表现得像下面这个 (2, 4) 的矩阵:</span></span><br><span class="line"><span class="comment"># [[0.1 0.2 0.3 0.4]</span></span><br><span class="line"><span class="comment">#  [0.1 0.2 0.3 0.4]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终结果 Z 的形状: (2, 4)</span></span><br><span class="line"><span class="comment"># 最终结果 Z 的值:</span></span><br><span class="line"><span class="comment">#  [[10.1 20.2 30.3 40.4]</span></span><br><span class="line"><span class="comment">#  [50.1 60.2 70.3 80.4]]</span></span><br></pre></td></tr></tbody></table></figure>
<p>我们可以清晰地看到 <code>B</code> 是如何被 “拉伸” 以匹配
<code>A_dot_W</code> 的形状，从而完成加法运算的。</p>
<img src="/Machine-learning/36604/6-matrices1.png" class="" title="matrices1">
<img src="/Machine-learning/36604/7-matrices2.png" class="" title="matrices2">
<h2 id="训练过程">训练过程</h2>
<p>我们已经知道数据是如何在网络中流动的 (前向传播)，但网络是如何 “学习”
—— 也就是如何找到正确的权重和偏置的呢？这个过程叫做<strong>训练
(Training)</strong>。</p>
<p>训练就像一个反馈循环：</p>
<ul>
<li><strong>预测</strong>：让网络根据当前的权重和偏置进行一次前向传播，得到一个预测结果。</li>
<li><strong>评估</strong>：将预测结果与真实的答案进行比较，计算出 “误差” 有多大。</li>
<li><strong>学习</strong>：根据误差，反向调整网络中的所有权重和偏置，目标是让下一次的误差变得更小。</li>
</ul>
<h3 id="损失函数">损失函数</h3>
<p>为了评估网络的表现，我们需要一个<strong>损失函数 (Loss
Function)</strong>。这会量化预测值和真实值之间的差距。</p>
<p>一个常用的损失函数是<strong>均方误差 (Mean Squared Error,
MSE)</strong>：</p>
<ul>
<li>计算每个输出神经元的预测值与真实值之差。</li>
<li>将这个差值平方（这样可以确保结果是正数，并且对较大的误差给予更大的 “惩罚”）。</li>
<li>将所有输出神经元的平方差加起来，并取平均值。</li>
</ul>
<p>即 <code>loss = mean((pred - true)²)</code></p>
<p>其数学公式为 (<span class="math inline"><em>k</em></span>
是输出神经元的数量):</p>
<p><span class="math display">$$ C = \frac{1}{m} \sum_{i=1}^{m}
\sum_{j=1}^{k} (y_{pred, ij} - y_{true, ij})^2 $$</span></p>
<p>用代码实现： </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y_true = np.array([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">y_pred = np.array([[<span class="number">0.1</span>, <span class="number">0.9</span>], [<span class="number">0.8</span>, <span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = mse_loss(y_true, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"loss = "</span>, loss)</span><br><span class="line"><span class="comment"># loss = 0.024999999999999994</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>我们的目标就是通过调整权重和偏置，让这个损失值<strong>尽可能小</strong>。</p>
<blockquote>
<p>实际上，对于分类任务，更常用的是 softmax
激活配合交叉熵损失（cross-entropy loss）。这里以 MSE
为例只是因为演示方便，后续章节会详细介绍分类场景的常用方法。</p>
</blockquote>
<img src="/Machine-learning/36604/8-cost.png" class="" title="cost">
<h3 id="梯度下降">梯度下降</h3>
<p>想象一下，你正站在一座大山 (损失函数)
上，蒙着眼睛，你的目标是走到山谷的最低点 (最小损失)。你会怎么做？
你可能会伸出脚，感受一下哪个方向是下坡最陡峭的，然后朝那个方向迈一小步。重复这个过程，你最终会到达谷底。</p>
<p>这就是<strong>梯度下降 (Gradient
Descent)</strong> 算法的核心思想。</p>
<ul>
<li><strong>梯度
(Gradient)</strong> 在数学上指向了函数值增长最快的方向 (也就是上坡最陡的方向)。</li>
<li>因此，我们只需要沿着<strong>负梯度</strong> (梯度的反方向) 前进，就能最快地降低损失值。
<img src="/Machine-learning/36604/9-gradientDescent.png" class="" title="gradientDescent"></li>
</ul>
<p>我们还需要一个<strong>学习率 (Learning
Rate)</strong>，它决定了我们沿着下坡方向 “迈出的步子有多大”。</p>
<ul>
<li><strong>学习率太小</strong>：下山速度会非常慢。</li>
<li><strong>学习率太大</strong>：我们可能会在谷底来回 “跨过”，永远无法到达最低点。</li>
</ul>
<p>选择一个合适的学习率是训练神经网络中的一个重要技巧。</p>
<p>参数更新的规则可以表示为： <span class="math display">$$ w_{new} =
w_{old} - \eta \frac{\partial C}{\partial w} $$</span> 其中
<code>η</code> 是学习率，<code>∂C/∂w</code> 是损失对权重 <code>w</code>
的梯度。</p>
<h3 id="反向传播">反向传播</h3>
<p>梯度下降告诉了我们要下山，但没有告诉我们网络中成千上万个权重和偏置具体要如何改变才能实现下山。</p>
<p><strong>反向传播
(Backpropagation)</strong> 就是解决这个问题的算法。它的核心思想是 “分配责任”。</p>
<p>在一次预测后，我们得到了总的误差。反向传播会从输出层开始，反向地通过网络，计算出每个权重和偏置对这个总误差 “贡献” 了多少责任。</p>
<h4 id="单个权重的的链式法则责任推导">单个权重的的链式法则责任推导</h4>
<p>这个过程依赖于微积分中的<strong>链式法则</strong>。我们想知道改变一个权重
<code>w</code> 会如何影响最终的损失 <code>C</code> (即梯度
<code>∂C/∂w</code>)。这个梯度可以被分解为几个部分的乘积：</p>
<p><span class="math display">$$ \frac{\partial C}{\partial w} =
\frac{\partial C}{\partial a} \times \frac{\partial a}{\partial z}
\times \frac{\partial z}{\partial w} $$</span></p>
<p>下面我们将拆解这个公式，把它想象成一个 “责任追踪” 的过程。我们的目标
<code>∂C/∂w</code> 是要算出权重 <code>w</code> 应该为最终的误差
<code>C</code> 负多少责任。</p>
<p>链式法则告诉我们，这个总责任可以分解为一连串局部责任的乘积：</p>
<ol type="1">
<li><strong><code>∂z/∂w</code>：权重 <code>w</code> 对神经元输入
<code>z</code> 的责任。</strong>
<ul>
<li>回顾公式 <code>z = w * a_prev + ...</code>，<code>w</code>
的影响大小，完全取决于它所乘的那个输入值 <code>a_prev</code>。如果
<code>a_prev</code> 很大，<code>w</code> 的一点小变化就会被放大；如果
<code>a_prev</code> 是 0，<code>w</code> 再怎么变也影响不了
<code>z</code>。<strong>所以，这部分的责任就是
<code>a_prev</code>。</strong></li>
</ul></li>
<li><strong><code>∂a/∂z</code>：神经元输入 <code>z</code> 对其输出
<code>a</code> 的责任。</strong>
<ul>
<li>对于 ReLU 函数，如果 <code>z</code> 本来就大于 0，那 <code>z</code>
的变化会直接通过，责任是 1。如果 <code>z</code> 小于等于 0，那
<code>z</code> 再怎么变，输出 <code>a</code>
都是 0，没有变化，那么责任是 0。<strong>所以，这部分的责任是激活函数的导数，它决定了梯度能否继续向后流动。</strong></li>
</ul></li>
<li><strong><code>∂C/∂a</code>：神经元输出 <code>a</code> 对最终总误差
<code>C</code> 的责任。</strong>
<ul>
<li>这是最关键的一步，也是 “反向传播” 中 “传播” 的体现。一个神经元的输出
<code>a</code>
会影响到下一层所有与它相连的神经元，进而通过整个网络，最终影响到总误差
<code>C</code>。我们无法直接计算这个责任。<strong>所以，这个责任值必须从下一层 “传播” 回来。</strong>
它代表了所有来自 “前方”（更靠近输出层）的误差信号汇集到 <code>a</code>
这一点上的总和。</li>
</ul></li>
</ol>
<p>综上，为了计算一个权重的梯度（<code>∂C/∂w</code>），我们把这三份 “责任” 相乘：(来自前一层的输入
<code>a_prev</code>) × (激活函数的导数) ×
(从后一层传回来的误差)。这就是反向传播的核心计算。</p>
<h4 id="多层神经网络的链式法则责任传递">多层神经网络的链式法则责任传递</h4>
<p>上述推导是针对单个权重的梯度，便于理解本质。而在实际代码实现中，一般用矩阵运算一次性计算整层的所有权重和偏置的梯度，这样程序的运算效率更高。代码中的
<code>dC_da</code>、<code>da_dz</code>、<code>dC_dz</code>、<code>dC_dw</code>、<code>dC_db</code>、<code>dC_da_prev</code>
等变量，都是批量（矩阵 / 向量）形式的 “责任” 传递，和理论推导一一对应，只是用矩阵方式高效实现。</p>
<p>在多层神经网络中，反向传播的 “责任” 会一层层传递。我们以第 <span class="math inline"><em>l</em></span> 层为例，假设：</p>
<ul>
<li><span class="math inline"><em>a</em><sup>[<em>l</em>]</sup></span>：第 <span class="math inline"><em>l</em></span> 层的输出（激活值）</li>
<li><span class="math inline"><em>z</em><sup>[<em>l</em>]</sup></span>：第 <span class="math inline"><em>l</em></span> 层的加权和</li>
<li><span class="math inline"><em> W</em><sup>[<em>l</em>]</sup></span>、<span class="math inline"><em>b</em><sup>[<em>l</em>]</sup></span>：第 <span class="math inline"><em>l</em></span> 层的权重和偏置</li>
</ul>
<p>前向传播： <span class="math display">$$
\begin{align}
z^{[l]} &amp;= W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} &amp;= f(z^{[l]})
\end{align}
$$</span></p>
<p>反向传播：</p>
<p>第一个 “误差信号” <span class="math inline">$\frac{\partial
C}{\partial
a^{[L]}}$</span>（损失对输出层激活的梯度）是直接由损失函数和真实标签计算得到。例如对于均方误差（MSE）损失
<span class="math inline">$C = \frac{1}{2}(a^{[L]} - y)^2$</span>，有
<span class="math inline">$\frac{\partial C}{\partial a^{[L]}} = a^{[L]}
- y$</span>。</p>
<p>“真实标签” 指的是训练数据中每个样本的正确答案，也叫 “目标值” 或 “ground
truth”。比如：</p>
<ul>
<li>图像分类任务中，真实标签就是图片实际对应的类别（如 “猫” 或 “狗”）。</li>
<li>回归任务中，真实标签就是希望模型预测出来的那个数值。</li>
</ul>
<p>在反向传播时，假设我们已经得到了 <span class="math inline">$\frac{\partial C}{\partial
a^{[l]}}$</span>，则：</p>
<ol type="1">
<li><p>计算 <span class="math inline">$\frac{\partial C}{\partial
z^{[l]}}$</span>。其中，符号 <span class="math inline">⊙</span>
表示逐元素乘法： <span class="math display">$$
\delta^{[l]} = \frac{\partial C}{\partial z^{[l]}} = \frac{\partial
C}{\partial a^{[l]}} \odot f'(z^{[l]})
$$</span></p></li>
<li><p> 计算本层参数的梯度，这里的 <span class="math inline"><em>m</em></span> 是 batch size： <span class="math display">$$
\begin{align}
\frac{\partial C}{\partial W^{[l]}} &amp;= \delta^{[l]} (a^{[l-1]})^T \\
\frac{\partial C}{\partial b^{[l]}} &amp;= \sum_{i=1}^m \delta^{[l]}_i
\end{align}
$$</span></p></li>
<li><p> 计算传递给前一层的 “责任”： <span class="math display">$$
\frac{\partial C}{\partial a^{[l-1]}} = (W^{[l]})^T \delta^{[l]}
$$</span></p></li>
</ol>
<p>这样，误差信号 <span class="math inline"><em>δ</em></span>
就一层层地传递回去。</p>
<img src="/Machine-learning/36604/10-backpropagation.png" class="" title="backpropagation">
<p><strong>变量对应关系</strong>：</p>
<table>
<colgroup>
<col style="width: 39%">
<col style="width: 19%">
<col style="width: 41%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">公式符号</th>
<th style="text-align: left;">代码变量名</th>
<th style="text-align: left;">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial a^{[l]}}$</span></td>
<td style="text-align: left;"><code>dC_da</code></td>
<td style="text-align: left;">损失对本层激活的梯度 (来自 <span class="math inline"><em>l</em> + 1</span> 层)</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"><em>f</em><sup>′</sup>(<em>z</em><sup>[<em>l</em>]</sup>)</span></td>
<td style="text-align: left;"><code>da_dz</code></td>
<td style="text-align: left;">激活函数对加权和的导数</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"><em> δ</em><sup>[<em>l</em>]</sup></span></td>
<td style="text-align: left;"><code>dC_dz</code></td>
<td style="text-align: left;">损失对本层加权和的梯度</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial W^{[l]}}$</span></td>
<td style="text-align: left;"><code>dC_dw</code></td>
<td style="text-align: left;">损失对本层权重的梯度</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial b^{[l]}}$</span></td>
<td style="text-align: left;"><code>dC_db</code></td>
<td style="text-align: left;">损失对本层偏置的梯度</td>
</tr>
<tr>
<td style="text-align: left;"><span class="math inline"> $\frac{\partial
C}{\partial a^{[l-1]}}$</span></td>
<td style="text-align: left;"><code>dC_da_prev</code></td>
<td style="text-align: left;">损失对前一层激活的梯度 (传递回 <span class="math inline"><em>l</em> − 1</span> 层)</td>
</tr>
</tbody>
</table>
<p>下面的代码段详细解释了每个变量的含义和它与理论公式的对应关系。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    执行反向传播，计算并应用梯度。</span></span><br><span class="line"><span class="string">    dC_da: 损失函数对本神经元输出 a 的梯度 (从下一层传来)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 1. 计算 da/dz (激活函数对z的梯度)</span></span><br><span class="line">    <span class="comment"># 对应链式法则中的 ∂a/∂z，形状与 z 相同</span></span><br><span class="line">    da_dz = <span class="variable language_">self</span>.relu_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 计算 dC/dz (损失对z的梯度) = dC/da * da/dz</span></span><br><span class="line">    <span class="comment"># 对应链式法则中的 ∂C/∂a × ∂a/∂z，形状与 z 相同</span></span><br><span class="line">    dC_dz = dC_da * da_dz</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 计算 dC/dw (损失对权重的梯度) = dC/dz * dz/dw</span></span><br><span class="line">    <span class="comment"># dz/dw = a_prev，dC/dw = a_prev.T @ dC/dz</span></span><br><span class="line">    <span class="comment"># 这里 a_prev 是输入，dC/dz 是“误差信号”，矩阵乘法一次性算出所有权重的梯度</span></span><br><span class="line">    dC_dw = <span class="variable language_">self</span>.last_input.T @ dC_dz</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 计算 dC/db (损失对偏置的梯度) = dC/dz * dz/db</span></span><br><span class="line">    <span class="comment"># dz/db = 1，dC/db = sum(dC/dz)</span></span><br><span class="line">    dC_db = np.<span class="built_in">sum</span>(dC_dz, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 计算 dC/da_prev (损失对前一层激活值的梯度)，用于传给前一层</span></span><br><span class="line">    <span class="comment"># dC/da_prev = dC/dz * dz/da_prev，dz/da_prev = weights</span></span><br><span class="line">    dC_da_prev = dC_dz @ <span class="variable language_">self</span>.weights.T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 根据梯度更新权重和偏置</span></span><br><span class="line">    <span class="variable language_">self</span>.weights -= learning_rate * dC_dw</span><br><span class="line">    <span class="variable language_">self</span>.bias -= learning_rate * dC_db</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回 dC/da_prev，传递给前一层继续反向传播</span></span><br><span class="line">    <span class="keyword">return</span> dC_da_prev</span><br></pre></td></tr></tbody></table></figure>
<h3 id="总结训练循环">总结训练循环</h3>
<p>于是，我们可以完整地描述训练过程了，这个过程会重复很多次 (称为
<strong>Epochs</strong>):</p>
<ol type="1">
<li><strong>前向传播</strong>：将一批训练数据输入网络，计算出预测值。</li>
<li><strong>计算损失</strong>：使用损失函数，比较预测值和真实值，得到损失。</li>
<li><strong>反向传播</strong>：从损失出发，反向计算出网络中每个权重和偏置的梯度。</li>
<li><strong>更新参数</strong>：使用梯度下降，根据梯度和学习率，对所有权重和偏置进行一次微小的更新。</li>
</ol>
<p>经过成千上万次的迭代，网络的权重和偏置会逐渐收敛到一组能够很好地完成任务的值。网络就 “学会” 了辨认猫猫。</p>
<h2 id="一个简单的-python-实现">一个简单的 Python 实现</h2>
<p>下面是一个用 Python 和 NumPy 从零开始实现的简单神经网络。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单个神经元。管理自己的权重、偏置，并执行前向和后向计算。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs</span>):</span><br><span class="line">        <span class="comment"># 随机初始化权重，乘以一个小数以防止初始值过大</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span></span><br><span class="line">        <span class="comment"># 初始化偏置为0</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 存储计算过程中的中间值，用于反向传播</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_z = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""ReLU 激活函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(<span class="number">0</span>, z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu_derivative</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""ReLU 激活函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.where(z &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行前向传播：z = a @ w + b, a_out = relu(z)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = activations</span><br><span class="line">        <span class="comment"># 计算加权和 z</span></span><br><span class="line">        z = activations @ <span class="variable language_">self</span>.weights + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="variable language_">self</span>.last_z = z</span><br><span class="line">        <span class="comment"># 应用激活函数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行反向传播，计算并应用梯度。</span></span><br><span class="line"><span class="string">        dC_da: 损失函数对本神经元输出 a 的梯度 (从下一层传来)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 1. 计算 da/dz (激活函数对z的梯度)</span></span><br><span class="line">        <span class="comment"># 对应链式法则中的 ∂a/∂z，形状与 z 相同</span></span><br><span class="line">        da_dz = <span class="variable language_">self</span>.relu_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算 dC/dz (损失对z的梯度) = dC/da * da/dz</span></span><br><span class="line">        <span class="comment"># 对应链式法则中的 ∂C/∂a × ∂a/∂z，形状与 z 相同</span></span><br><span class="line">        dC_dz = dC_da * da_dz</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 计算 dC/dw (损失对权重的梯度) = dC/dz * dz/dw</span></span><br><span class="line">        <span class="comment"># dz/dw = a_prev，dC/dw = a_prev.T @ dC/dz</span></span><br><span class="line">        <span class="comment"># 这里 a_prev 是输入，dC/dz 是“误差信号”，矩阵乘法一次性算出所有权重的梯度</span></span><br><span class="line">        dC_dw = <span class="variable language_">self</span>.last_input.T @ dC_dz</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 计算 dC/db (损失对偏置的梯度) = dC/dz * dz/db</span></span><br><span class="line">        <span class="comment"># dz/db = 1，dC/db = sum(dC/dz)</span></span><br><span class="line">        dC_db = np.<span class="built_in">sum</span>(dC_dz, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 计算 dC/da_prev (损失对前一层激活值的梯度)，用于传给前一层</span></span><br><span class="line">        <span class="comment"># dC/da_prev = dC/dz * dz/da_prev，dz/da_prev = weights</span></span><br><span class="line">        dC_da_prev = dC_dz @ <span class="variable language_">self</span>.weights.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 根据梯度更新权重和偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.weights -= learning_rate * dC_dw</span><br><span class="line">        <span class="variable language_">self</span>.bias -= learning_rate * dC_db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回 dC/da_prev，传递给前一层继续反向传播</span></span><br><span class="line">        <span class="keyword">return</span> dC_da_prev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一层神经元。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_neurons, num_inputs_per_neuron</span>):</span><br><span class="line">        <span class="variable language_">self</span>.neurons = [Neuron(num_inputs_per_neuron) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_neurons)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""对层中所有神经元执行前向传播"""</span></span><br><span class="line">        <span class="comment"># hstack 用于水平堆叠输出，形成一个 (batch_size, num_neurons) 的矩阵</span></span><br><span class="line">        <span class="keyword">return</span> np.hstack([neuron.forward(activations) <span class="keyword">for</span> neuron <span class="keyword">in</span> <span class="variable language_">self</span>.neurons])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">        <span class="string">"""对层中所有神经元执行反向传播"""</span></span><br><span class="line">        <span class="comment"># dC_da 的形状是 (batch_size, num_neurons)</span></span><br><span class="line">        <span class="comment"># 我们需要为每个神经元传入对应的梯度 dC_da[:, [i]]</span></span><br><span class="line">        <span class="comment"># 然后将所有神经元返回的 dC/da_prev 相加，得到传给前一层的总梯度</span></span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(</span><br><span class="line">            [</span><br><span class="line">                neuron.backward(dC_da[:, [i]], learning_rate)</span><br><span class="line">                <span class="keyword">for</span> i, neuron <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.neurons)</span><br><span class="line">            ],</span><br><span class="line">            axis=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    完整的神经网络模型。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_sizes</span>):</span><br><span class="line">        <span class="comment"># layer_sizes 是一个列表，例如 [784, 128, 10]</span></span><br><span class="line">        <span class="comment"># 表示输入层784个节点，隐藏层128个，输出层10个</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(Layer(layer_sizes[i + <span class="number">1</span>], layer_sizes[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""对所有层执行前向传播"""</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            activations = layer.forward(activations)</span><br><span class="line">        <span class="keyword">return</span> activations</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="string">"""均方误差损失函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">derivative_mse_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="string">"""均方误差损失函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * (y_pred - y_true) / y_true.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs, learning_rate, batch_size=<span class="number">32</span></span>):</span><br><span class="line">        <span class="string">"""训练神经网络"""</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 使用小批量梯度下降</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(X), batch_size):</span><br><span class="line">                X_batch = X[i : i + batch_size]</span><br><span class="line">                y_batch = y[i : i + batch_size]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 1. 前向传播</span></span><br><span class="line">                outputs = <span class="variable language_">self</span>.forward(X_batch)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 2. 计算损失</span></span><br><span class="line">                loss = <span class="variable language_">self</span>.mse_loss(y_batch, outputs)</span><br><span class="line">                total_loss += loss * <span class="built_in">len</span>(X_batch)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 3. 计算输出层的梯度</span></span><br><span class="line">                output_gradient = <span class="variable language_">self</span>.derivative_mse_loss(y_batch, outputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 4. 反向传播</span></span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.layers):</span><br><span class="line">                    output_gradient = layer.backward(output_gradient, learning_rate)</span><br><span class="line"></span><br><span class="line">            avg_loss = total_loss / <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{epochs}</span>, Loss: <span class="subst">{avg_loss:<span class="number">.6</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">"""用训练好的网络进行预测"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.forward(X)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>Ian Goodfellow, Yoshua Bengio, Aaron Courville. <a href="https://www.deeplearningbook.org/">Deep Learning</a></li>
<li><a href="https://github.com/DorsaRoh/Machine-Learning">Machine
learning - DorsaRoh</a></li>
</ul>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始的神经网络学习 (二)：实用技巧与进阶</title>
    <url>/Machine-learning/38196.html</url>
    <content><![CDATA[<p>在<a href="https://0wnerdied.github.io/Machine-learning/36604.html">上一篇文章</a>中，我们从零开始构建了一个简单的神经网络，并理解了前向传播、反向传播和梯度下降等核心概念。然而，要让神经网络在现实世界的问题中高效工作，我们还需要掌握更多的工具和技巧。</p>
<p>这篇文章将作为第二部分，专注于第一部分中未能详尽涵盖的几个关键领域：</p>
<ul>
<li>我将用上一篇文章中的代码来训练一个网络，解决一个经典问题，并观察损失函数的变化。</li>
<li>除了 ReLU，我还将介绍 Sigmoid 和 Tanh
等其他常用激活函数，并讨论如何为输出层选择合适的激活函数。</li>
<li>探讨权重初始化的重要性，并介绍批量归一化 (Batch Normalization) 和
Dropout 等强大的技术。</li>
<li>最后，分享一些关于调试神经网络和选择超参数的实用技巧。</li>
</ul>
<h2 id="训练一个-xor-网络">训练一个 XOR 网络</h2>
<p>理论需要实践来检验。让我们使用上一篇文章中定义的
<code>NeuralNetwork</code> 类来解决经典的 XOR 问题。XOR
是一个非线性问题，单个神经元无法解决，因此很适合作为我们神经网络的测试案例。</p>
<p>XOR 的真值表如下：</p>
<table>
<thead>
<tr>
<th style="text-align: center;">输入 A</th>
<th style="text-align: center;"> 输入 B</th>
<th style="text-align: center;"> 输出</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"> 0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<h3 id="准备数据和网络">准备数据和网络</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># existing codes...</span></span><br><span class="line">            avg_loss = total_loss / <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="comment"># 只在每1/10进度时输出一次</span></span><br><span class="line">            <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="built_in">max</span>(<span class="number">1</span>, epochs // <span class="number">10</span>) == <span class="number">0</span> <span class="keyword">or</span> epoch == <span class="number">0</span> <span class="keyword">or</span> epoch == epochs - <span class="number">1</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{epochs}</span>, Loss: <span class="subst">{avg_loss:<span class="number">.6</span>f}</span>"</span>)</span><br><span class="line"><span class="comment"># existing codes...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义 XOR 数据集</span></span><br><span class="line">X_train = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y_train = np.array([[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建神经网络实例</span></span><br><span class="line"><span class="comment"># 2个输入节点，一个有2个节点的隐藏层，1个输出节点</span></span><br><span class="line">nn = NeuralNetwork([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="训练并观察损失">训练并观察损失</h3>
<p>我们将使用较小的学习率和足够多的迭代次数来训练网络。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 3. 训练网络</span></span><br><span class="line"><span class="comment"># XOR 是一个非线性问题，需要足够的迭代次数和合适的学习率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"开始训练XOR网络..."</span>)</span><br><span class="line">nn.train(X_train, y_train, epochs=<span class="number">10000</span>, learning_rate=<span class="number">0.005</span>, batch_size=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"训练完成。"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练XOR网络...</span></span><br><span class="line"><span class="comment"># Epoch 1/10000, Loss: 0.499970</span></span><br><span class="line"><span class="comment"># Epoch 1000/10000, Loss: 0.249839</span></span><br><span class="line"><span class="comment"># Epoch 2000/10000, Loss: 0.249042</span></span><br><span class="line"><span class="comment"># Epoch 3000/10000, Loss: 0.244619</span></span><br><span class="line"><span class="comment"># Epoch 4000/10000, Loss: 0.224387</span></span><br><span class="line"><span class="comment"># Epoch 5000/10000, Loss: 0.177830</span></span><br><span class="line"><span class="comment"># Epoch 6000/10000, Loss: 0.113382</span></span><br><span class="line"><span class="comment"># Epoch 7000/10000, Loss: 0.023213</span></span><br><span class="line"><span class="comment"># Epoch 8000/10000, Loss: 0.001769</span></span><br><span class="line"><span class="comment"># Epoch 9000/10000, Loss: 0.000104</span></span><br><span class="line"><span class="comment"># Epoch 10000/10000, Loss: 0.000006</span></span><br><span class="line"><span class="comment"># 训练完成。</span></span><br></pre></td></tr></tbody></table></figure>
<p>正如我们所见，损失 (Loss)
随着训练的进行而稳步下降，这表明我们的网络确实在学习如何解决 XOR
问题。</p>
<h3 id="查看结果">查看结果</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 4. 进行预测并展示结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n对输入进行预测:"</span>)</span><br><span class="line"><span class="keyword">for</span> x_input <span class="keyword">in</span> X_train:</span><br><span class="line">    prediction = nn.predict(x_input.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"输入: <span class="subst">{x_input}</span>, 预测输出: <span class="subst">{prediction[<span class="number">0</span>][<span class="number">0</span>]:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对输入进行预测:</span></span><br><span class="line"><span class="comment"># 输入: [0 0], 预测输出: 0.0027</span></span><br><span class="line"><span class="comment"># 输入: [0 1], 预测输出: 0.9981</span></span><br><span class="line"><span class="comment"># 输入: [1 0], 预测输出: 0.9981</span></span><br><span class="line"><span class="comment"># 输入: [1 1], 预测输出: 0.0028</span></span><br></pre></td></tr></tbody></table></figure>
<p><del>预测值非常接近真实值，证明这个简单的神经网络框架是有效的。</del>
其实这个结果是我精挑细选，训练了很多很多次才得到的成功预测结果，训练其实是有随机性的，上面的代码在我本地测试中成功率非常低，训练十次都不一定能有一次成功，极大概率会训练失败，可以说这是一个失败的网络实现。这时就要调整参数或者更换合适的激活函数。</p>
<h2 id="其它的激活函数">其它的激活函数</h2>
<p>前面我们只介绍了
ReLU。虽然它非常流行且有效，但了解其他激活函数以及如何为输出层做选择也同样重要。</p>
<h3 id="sigmoid">Sigmoid</h3>
<p>Sigmoid 函数将任意实数压缩到 (0, 1)
区间内，所以它很适合用来表示概率。</p>
<ul>
<li><p><strong>公式</strong>: <span class="math display">$$ \sigma(z) =
\frac{1}{1 + e^{-z}} $$</span></p></li>
<li><p><strong>Python 实现</strong>: </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sigmoid(np.array([-<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="comment"># [0.11920292 0.5        0.88079708]</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p><strong>优点</strong>: 输出在 (0, 1)
之间，平滑且易于求导。</p></li>
<li><p><strong>缺点</strong>:</p>
<ul>
<li><strong>梯度消失</strong>:
当输入非常大或非常小时，函数的导数趋近于 0，导致梯度在反向传播时消失，使网络难以训练。</li>
<li><strong>输出不以 0 为中心</strong>:
输出总是正数，这可能导致后续层权重更新时朝同一个方向移动，降低收敛速度。</li>
</ul></li>
</ul>
<h3 id="tanh-双曲正切">Tanh (双曲正切)</h3>
<p>Tanh 函数会将输入压缩到 (-1, 1) 区间。</p>
<ul>
<li><p><strong>公式</strong>: <span class="math display">$$ tanh(z) =
\frac{e^z - e^{-z}}{e^z + e^{-z}} $$</span></p></li>
<li><p><strong>Python 实现</strong>: </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tanh(np.array([-<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="comment"># [-0.96402758  0.          0.96402758]</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p><strong>优点</strong>:</p>
<ul>
<li><strong>以 0 为中心</strong>: 输出在 -1 和 1 之间，解决了 Sigmoid
的一个主要缺点。</li>
<li>通常比 Sigmoid 收敛更快。</li>
</ul></li>
<li><p><strong>缺点</strong>: 仍然存在梯度消失的问题。</p></li>
</ul>
<h3 id="如何为输出层选择激活函数">如何为输出层选择激活函数？</h3>
<p>输出层的激活函数选择至关重要，因为它决定了网络输出的格式。</p>
<ul>
<li><p><strong>二元分类 (Binary Classification)</strong>:
当你预测两个类别之一时（例如，是猫 / 不是猫），使用
<strong>Sigmoid</strong>
函数。它输出一个 0 到 1 之间的值，可以解释为属于正类的概率。</p></li>
<li><p><strong>多元分类 (Multi-class Classification)</strong>:
当你在多个类别中选择一个时（例如，数字识别 0-9），使用
<strong>Softmax</strong>
函数。它能将一组数字转换成概率分布，所有输出的总和为 1。</p></li>
<li><p><strong>Softmax 实现</strong>: </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">z</span>):</span><br><span class="line">    exp_z = np.exp(z - np.<span class="built_in">max</span>(z, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> exp_z / np.<span class="built_in">sum</span>(exp_z, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(softmax(np.array([[<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>]])))</span><br><span class="line"><span class="comment"># [[0.65900114 0.24243297 0.09856589]]</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p><strong>回归 (Regression)</strong>:
当预测一个连续值时（例如，房价），输出层<strong>不使用任何激活函数</strong>。这样，网络就可以输出任意范围的数值。</p></li>
</ul>
<h2 id="优化与正则化技巧">优化与正则化技巧</h2>
<p>为了构建更强大、更稳定的神经网络，我们需要一些高级的优化和正则化技术。</p>
<h3 id="权重初始化的重要性">权重初始化的重要性</h3>
<p>我们在第一部分中用简单的 <code>np.random.randn() * 0.01</code>
来初始化权重。这虽然行得通，但不是最好的解决方案。糟糕的权重初始化可能导致梯度消失或梯度爆炸，即梯度在反向传播过程中变得过小或过大。</p>
<p>现代的初始化方法，如 <strong>Xavier (Glorot) 初始化</strong> 和
<strong>He
初始化</strong>，通过智能地根据上一层的神经元数量来调整初始权重的方差，从而确保信号在网络中更稳定地传播，显著加快训练速度并提高性能。</p>
<ul>
<li><p><strong>Xavier 初始化</strong>: 通常与 Sigmoid 或 Tanh
激活函数配合使用。</p></li>
<li><p><strong>He 初始化</strong>: 专为 ReLU
及其变体设计，是现代深度网络中的首选。</p></li>
<li><p><strong>Xavier/He 初始化示例</strong>: </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Xavier 初始化 (适合Sigmoid/Tanh)</span></span><br><span class="line">fan_in, fan_out = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">xavier = np.random.randn(fan_in, fan_out) * np.sqrt(<span class="number">1.0</span> / fan_in)</span><br><span class="line"><span class="comment"># He 初始化 (适合ReLU)</span></span><br><span class="line">he = np.random.randn(fan_in, fan_out) * np.sqrt(<span class="number">2.0</span> / fan_in)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ul>
<h3 id="批量归一化-batch-normalization">批量归一化 (Batch
Normalization)</h3>
<p>在每个小批量数据通过网络时，对每一层的输入进行归一化（调整为均值为 0，方差为 1），然后再进行缩放和平移。</p>
<p><strong>好处</strong>:</p>
<ul>
<li><p><strong>加速训练</strong>: 允许使用更高的学习率。</p></li>
<li><p><strong>稳定训练</strong>: 减少了对权重初始化的敏感度。</p></li>
<li><p><strong>轻微的正则化效果</strong>:
由于是在小批量上计算均值和方差，引入的噪声可以起到类似 Dropout
的效果。</p></li>
<li><p><strong>BatchNorm 示例</strong>: </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">x, gamma, beta, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">    mu = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">    var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line">    x_norm = (x - mu) / np.sqrt(var + eps)</span><br><span class="line">    <span class="keyword">return</span> gamma * x_norm + beta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># x: (batch_size, features)</span></span><br><span class="line"><span class="comment"># gamma/beta: 可学习参数，初始为1和0</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ul>
<h3 id="dropout">Dropout</h3>
<p>Dropout
是一种简单而有效的正则化技术，用于防止网络过拟合。在训练过程中的每一步，它会以一定的概率
<code>p</code> 随机地 “丢弃” 网络中的一部分神经元。</p>
<p>这意味着网络不能依赖于任何一个特定的神经元，迫使它学习到更健壮、更冗余的特征表示。在测试时，所有神经元都会被使用，但它们的输出会按比例
<code>(1-p)</code> 缩小，以平衡训练时的丢弃行为。</p>
<ul>
<li><strong>Dropout 示例</strong>: <figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout</span>(<span class="params">x, p</span>):</span><br><span class="line">    mask = (np.random.rand(*x.shape) &gt; p).astype(<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">return</span> x * mask / (<span class="number">1</span> - p)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练时: x = dropout(x, p=0.5)</span></span><br><span class="line"><span class="comment"># 推理时: 不用dropout</span></span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h2 id="损失函数的选择">损失函数的选择</h2>
<p>在二元分类任务中，输出层通常用 sigmoid
激活，最合适的损失函数是<strong>二元交叉熵（Binary Cross Entropy,
BCE）</strong>，而不是均方误差（MSE）。</p>
<ul>
<li><p><strong>BCE 公式</strong>，其中 <span class="math inline"><em>y</em></span> 是真实标签（0 或 1），<span class="math inline"><em>p</em></span> 是 sigmoid 输出概率： <span class="math display"><em>L</em> = −[<em>y</em> ⋅ ln <em>p</em> + (1 − <em>y</em>) ⋅ ln (1 − <em>p</em>)]</span></p></li>
<li><p><strong>BCE 导数</strong>： <span class="math display">$$
\frac{\partial L}{\partial p} = -\frac{y}{p} + \frac{1-y}{1-p}
$$</span></p></li>
<li><p><strong> 区别</strong>：</p>
<ul>
<li>MSE 在 sigmoid 饱和区间梯度更容易消失，收敛慢。</li>
<li>BCE 更适合概率输出，收敛快。</li>
</ul></li>
</ul>
<h2 id="建议">建议</h2>
<h3 id="调试神经网络的技巧">调试神经网络的技巧</h3>
<p>当神经网络不工作时，调试让人<del>心旷神怡</del>。下面是一些实用的检查步骤：</p>
<ol type="1">
<li>先用一个非常小的网络（例如一个隐藏层，少量神经元）来<strong>过拟合</strong>一小部分训练数据（例如，仅 10-20 个样本）。如果连这一步都做不到，说明模型结构或代码实现有根本性问题。</li>
</ol>
<ul>
<li><strong>过拟合小数据集</strong>: <figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># X_small, y_small = 10个样本</span></span><br><span class="line">nn = NeuralNetwork([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">nn.train(X_small, y_small, epochs=<span class="number">5000</span>, learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 观察loss是否能降到极低</span></span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<ol start="2" type="1">
<li><p>确保输入数据 <code>X</code> 和标签 <code>y</code>
是正确配对的。可以对数据进行可视化，检查是否存在异常值或错误。确保数据已经正确归一化。</p></li>
<li><p>一个太高的学习率是导致损失爆炸或不收敛的最常见原因。尝试将学习率降低一个数量级（例如从
<code>0.01</code> 到 <code>0.001</code>）。</p></li>
</ol>
<ul>
<li><strong>学习率调参</strong>: <figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> [<span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>]:</span><br><span class="line">    nn.train(X, y, epochs=<span class="number">1000</span>, learning_rate=lr)</span><br><span class="line">    <span class="comment"># 观察loss曲线</span></span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<ol start="4" type="1">
<li>确保选择了正确的损失函数，并且在反向传播时正确计算了对应的导数。</li>
</ol>
<h3 id="超参数的选择">超参数的选择</h3>
<p>超参数是在训练开始前设置的参数，例如学习率、层数等。</p>
<ul>
<li><strong>学习率 (Learning Rate)</strong>:
学习率是最重要的超参数。通常从 <code>0.1</code>, <code>0.01</code>,
<code>0.001</code> 等值开始尝试。可以使用<strong>学习率衰减 (Learning
Rate Decay)</strong>，即在训练过程中逐渐降低学习率。</li>
<li><strong>网络架构 (层数和神经元数量)</strong>:
从一个隐藏层开始。如果网络无法很好地拟合训练数据，再逐步增加层的深度和 / 或宽度。通常，增加深度比增加宽度更有效。</li>
<li><strong>批量大小 (Batch Size)</strong>: 以前通常选择 2 的幂，如 32,
64, 128，但现在的说法是，随便选什么数都行。比如，你可以选 520
训练一个神经网络，送给你的对象（笑
<ul>
<li><strong>小批量</strong>: 训练速度快，引入的噪声可能有助于泛化。</li>
<li><strong>大批量</strong>:
梯度估计更准确，但可能陷入局部最小值，且需要更多内存。</li>
</ul></li>
<li><strong>优化器 (Optimizer)</strong>:
我们只讨论了基本的梯度下降。现代优化器如 <strong>Adam</strong>,
<strong>RMSprop</strong>
通常能提供更快的收敛速度和更好的性能，它们会自动调整学习率。在平常实践中，Adam
是一个非常好的默认选择。</li>
</ul>
<h2 id="优化-xor-网络">优化 XOR 网络</h2>
<p>相信看完上面的内容后，我们对神经网络有了更多的了解。现在对 XOR
网络的代码添加以下优化：</p>
<ul>
<li>Neuron 支持 tanh、sigmoid、relu 三种激活函数。</li>
<li>隐藏层用 tanh，输出层用 sigmoid。</li>
<li>权重初始化方式根据激活函数自动选择（tanh/sigmoid 用 Xavier，relu 用
He）。</li>
<li>训练和预测流程自动适配。</li>
</ul>
<p>下面是优化后的 XOR 神经网络训练完整代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单个神经元。支持多种激活函数。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, activation=<span class="string">"relu"</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.activation_name = activation</span><br><span class="line">        <span class="comment"># 根据激活函数选择初始化方式</span></span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">            <span class="variable language_">self</span>.weights = np.random.randn(num_inputs, <span class="number">1</span>) * np.sqrt(<span class="number">2.0</span> / num_inputs)</span><br><span class="line">        <span class="keyword">elif</span> activation <span class="keyword">in</span> (<span class="string">"tanh"</span>, <span class="string">"sigmoid"</span>):</span><br><span class="line">            <span class="variable language_">self</span>.weights = np.random.randn(num_inputs, <span class="number">1</span>) * np.sqrt(<span class="number">1.0</span> / num_inputs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"不支持的激活函数: <span class="subst">{activation}</span>"</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bias = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_z = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_a = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""ReLU 激活函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(<span class="number">0</span>, z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu_derivative</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""ReLU 激活函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.where(z &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""Tanh 激活函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.tanh(z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tanh_derivative</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""Tanh 激活函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - np.tanh(z) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""Sigmoid 激活函数"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid_derivative</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="string">"""Sigmoid 激活函数的导数"""</span></span><br><span class="line">        s = <span class="variable language_">self</span>.sigmoid(z)</span><br><span class="line">        <span class="keyword">return</span> s * (<span class="number">1</span> - s)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行前向传播：z = a * w + b, a_out = 激活函数(z)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = activations</span><br><span class="line">        <span class="comment"># 计算加权和 z</span></span><br><span class="line">        z = activations @ <span class="variable language_">self</span>.weights + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="variable language_">self</span>.last_z = z</span><br><span class="line">        <span class="comment"># 应用激活函数</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.activation_name == <span class="string">"relu"</span>:</span><br><span class="line">            a = <span class="variable language_">self</span>.relu(z)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.activation_name == <span class="string">"tanh"</span>:</span><br><span class="line">            a = <span class="variable language_">self</span>.tanh(z)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.activation_name == <span class="string">"sigmoid"</span>:</span><br><span class="line">            a = <span class="variable language_">self</span>.sigmoid(z)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"不支持的激活函数: <span class="subst">{self.activation_name}</span>"</span>)</span><br><span class="line">        <span class="variable language_">self</span>.last_a = a</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行反向传播，计算并应用梯度。</span></span><br><span class="line"><span class="string">        dC_da: 损失函数对本神经元输出 a 的梯度 (从下一层传来)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 1. 计算 da/dz (激活函数对z的梯度)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.activation_name == <span class="string">"relu"</span>:</span><br><span class="line">            da_dz = <span class="variable language_">self</span>.relu_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.activation_name == <span class="string">"tanh"</span>:</span><br><span class="line">            da_dz = <span class="variable language_">self</span>.tanh_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.activation_name == <span class="string">"sigmoid"</span>:</span><br><span class="line">            da_dz = <span class="variable language_">self</span>.sigmoid_derivative(<span class="variable language_">self</span>.last_z)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"不支持的激活函数: <span class="subst">{self.activation_name}</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算 dC/dz (损失对z的梯度) = dC/da * da/dz</span></span><br><span class="line">        dC_dz = dC_da * da_dz</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 计算 dC/dw (损失对权重的梯度) = dC/dz * dz/dw</span></span><br><span class="line">        <span class="comment">#    dz/dw = last_input, 所以 dC/dw = last_input.T * dC/dz</span></span><br><span class="line">        dC_dw = <span class="variable language_">self</span>.last_input.T @ dC_dz</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 计算 dC/db (损失对偏置的梯度) = dC/dz * dz/db</span></span><br><span class="line">        <span class="comment">#    dz/db = 1, 所以 dC/db = dC/dz</span></span><br><span class="line">        dC_db = np.<span class="built_in">sum</span>(dC_dz, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 计算 dC/da_prev (损失对前一层激活值的梯度)，用于传给前一层</span></span><br><span class="line">        <span class="comment">#    dC/da_prev = dC/dz * dz/da_prev</span></span><br><span class="line">        <span class="comment">#    dz/da_prev = weights, 所以 dC/da_prev = dC/dz * weights.T</span></span><br><span class="line">        dC_da_prev = dC_dz @ <span class="variable language_">self</span>.weights.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 根据梯度更新权重和偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.weights -= learning_rate * dC_dw</span><br><span class="line">        <span class="variable language_">self</span>.bias -= learning_rate * dC_db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回 dC/da_prev，传递给前一层继续反向传播</span></span><br><span class="line">        <span class="keyword">return</span> dC_da_prev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一层神经元。支持指定激活函数。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_neurons, num_inputs_per_neuron, activation=<span class="string">"relu"</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.neurons = [</span><br><span class="line">            Neuron(num_inputs_per_neuron, activation) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_neurons)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""对层中所有神经元执行前向传播"""</span></span><br><span class="line">        <span class="comment"># hstack 用于水平堆叠输出，形成一个 (batch_size, num_neurons) 的矩阵</span></span><br><span class="line">        <span class="keyword">return</span> np.hstack([neuron.forward(activations) <span class="keyword">for</span> neuron <span class="keyword">in</span> <span class="variable language_">self</span>.neurons])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dC_da, learning_rate</span>):</span><br><span class="line">        <span class="string">"""对层中所有神经元执行反向传播"""</span></span><br><span class="line">        <span class="comment"># dC_da 的形状是 (batch_size, num_neurons)</span></span><br><span class="line">        <span class="comment"># 我们需要为每个神经元传入对应的梯度 dC_da[:, [i]]</span></span><br><span class="line">        <span class="comment"># 然后将所有神经元返回的 dC/da_prev 相加，得到传给前一层的总梯度</span></span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(</span><br><span class="line">            [</span><br><span class="line">                neuron.backward(dC_da[:, [i]], learning_rate)</span><br><span class="line">                <span class="keyword">for</span> i, neuron <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.neurons)</span><br><span class="line">            ],</span><br><span class="line">            axis=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    完整的神经网络模型。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_sizes, activations=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># activations: 每层的激活函数（不含输入层），如 ["tanh", "sigmoid"]</span></span><br><span class="line">        <span class="keyword">if</span> activations <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 默认隐藏层tanh，输出层sigmoid</span></span><br><span class="line">            activations = [<span class="string">"tanh"</span>] * (<span class="built_in">len</span>(layer_sizes) - <span class="number">2</span>) + [<span class="string">"sigmoid"</span>]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(activations) == <span class="built_in">len</span>(layer_sizes) - <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(</span><br><span class="line">                Layer(layer_sizes[i + <span class="number">1</span>], layer_sizes[i], activations[i])</span><br><span class="line">            )</span><br><span class="line">        <span class="variable language_">self</span>.output_activation = activations[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, activations</span>):</span><br><span class="line">        <span class="string">"""对所有层执行前向传播"""</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            activations = layer.forward(activations)</span><br><span class="line">        <span class="keyword">return</span> activations</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="string">"""均方误差损失函数"""</span></span><br><span class="line">        <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">derivative_mse_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="string">"""均方误差损失函数的导数"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * (y_pred - y_true) / y_true.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bce_loss</span>(<span class="params">self, y_true, y_pred, eps=<span class="number">1e-8</span></span>):</span><br><span class="line">        <span class="string">"""二元交叉熵损失函数"""</span></span><br><span class="line">        y_pred = np.clip(y_pred, eps, <span class="number">1</span> - eps)</span><br><span class="line">        <span class="keyword">return</span> -np.mean(y_true * np.log(y_pred) + (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> - y_pred))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">derivative_bce_loss</span>(<span class="params">self, y_true, y_pred, eps=<span class="number">1e-8</span></span>):</span><br><span class="line">        <span class="string">"""二元交叉熵损失函数的导数"""</span></span><br><span class="line">        y_pred = np.clip(y_pred, eps, <span class="number">1</span> - eps)</span><br><span class="line">        <span class="keyword">return</span> (y_pred - y_true) / (y_pred * (<span class="number">1</span> - y_pred) * y_true.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs, learning_rate, batch_size=<span class="number">32</span></span>):</span><br><span class="line">        <span class="string">"""训练神经网络"""</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(X), batch_size):</span><br><span class="line">                X_batch = X[i : i + batch_size]</span><br><span class="line">                y_batch = y[i : i + batch_size]</span><br><span class="line"></span><br><span class="line">                outputs = <span class="variable language_">self</span>.forward(X_batch)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 自动选择损失函数</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.output_activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">                    loss = <span class="variable language_">self</span>.bce_loss(y_batch, outputs)</span><br><span class="line">                    output_gradient = <span class="variable language_">self</span>.derivative_bce_loss(y_batch, outputs)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    loss = <span class="variable language_">self</span>.mse_loss(y_batch, outputs)</span><br><span class="line">                    output_gradient = <span class="variable language_">self</span>.derivative_mse_loss(y_batch, outputs)</span><br><span class="line"></span><br><span class="line">                total_loss += loss * <span class="built_in">len</span>(X_batch)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="variable language_">self</span>.layers):</span><br><span class="line">                    output_gradient = layer.backward(output_gradient, learning_rate)</span><br><span class="line"></span><br><span class="line">            avg_loss = total_loss / <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="keyword">if</span> (</span><br><span class="line">                (epoch + <span class="number">1</span>) % <span class="built_in">max</span>(<span class="number">1</span>, epochs // <span class="number">10</span>) == <span class="number">0</span></span><br><span class="line">                <span class="keyword">or</span> epoch == <span class="number">0</span></span><br><span class="line">                <span class="keyword">or</span> epoch == epochs - <span class="number">1</span></span><br><span class="line">            ):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>/<span class="subst">{epochs}</span>, Loss: <span class="subst">{avg_loss:<span class="number">.6</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.forward(X)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.output_activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">            <span class="keyword">return</span> out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义 XOR 数据集</span></span><br><span class="line">X_train = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y_train = np.array([[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建神经网络实例</span></span><br><span class="line"><span class="comment"># 2个输入节点，一个有2个节点的隐藏层，1个输出节点，激活函数分别为tanh和sigmoid</span></span><br><span class="line">nn = NeuralNetwork([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], activations=[<span class="string">"tanh"</span>, <span class="string">"sigmoid"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练网络</span></span><br><span class="line"><span class="comment"># XOR 是一个非线性问题，需要足够的迭代次数和合适的学习率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"开始训练XOR网络..."</span>)</span><br><span class="line">nn.train(X_train, y_train, epochs=<span class="number">200000</span>, learning_rate=<span class="number">0.1</span>, batch_size=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"训练完成。"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 进行预测并展示结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n对输入进行预测:"</span>)</span><br><span class="line"><span class="keyword">for</span> x_input <span class="keyword">in</span> X_train:</span><br><span class="line">    prediction = nn.predict(x_input.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"输入: <span class="subst">{x_input}</span>, 预测输出: <span class="subst">{prediction[<span class="number">0</span>][<span class="number">0</span>]:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练XOR网络...</span></span><br><span class="line"><span class="comment"># Epoch 1/200000, Loss: 0.710580</span></span><br><span class="line"><span class="comment"># Epoch 20000/200000, Loss: 0.001633</span></span><br><span class="line"><span class="comment"># Epoch 40000/200000, Loss: 0.000789</span></span><br><span class="line"><span class="comment"># Epoch 60000/200000, Loss: 0.000520</span></span><br><span class="line"><span class="comment"># Epoch 80000/200000, Loss: 0.000387</span></span><br><span class="line"><span class="comment"># Epoch 100000/200000, Loss: 0.000308</span></span><br><span class="line"><span class="comment"># Epoch 120000/200000, Loss: 0.000256</span></span><br><span class="line"><span class="comment"># Epoch 140000/200000, Loss: 0.000219</span></span><br><span class="line"><span class="comment"># Epoch 160000/200000, Loss: 0.000191</span></span><br><span class="line"><span class="comment"># Epoch 180000/200000, Loss: 0.000170</span></span><br><span class="line"><span class="comment"># Epoch 200000/200000, Loss: 0.000153</span></span><br><span class="line"><span class="comment"># 训练完成。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对输入进行预测:</span></span><br><span class="line"><span class="comment"># 输入: [0 0], 预测输出: 0.0002</span></span><br><span class="line"><span class="comment"># 输入: [0 1], 预测输出: 0.9999</span></span><br><span class="line"><span class="comment"># 输入: [1 0], 预测输出: 0.9999</span></span><br><span class="line"><span class="comment"># 输入: [1 1], 预测输出: 0.0002</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以明显看到 XOR
网络的预测更加精准了，并且我在本地连续训练了很多次，几乎不再有训练失败的情况发生。显然，我们的神经网络实现可以说成功了。</p>
<p>本篇中的 Layer/Neuron 结构采用 for-loop +
np.hstack，便于理解原理。实际程序中建议采用全矩阵化实现以提升效率。</p>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
</search>
